# web_news.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
import os
import google.generativeai as genai
from newspaper import Article

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨", "ê³ ìš©ë…¸ë™ë¶€", "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨"]
DATA_DIR = Path("data")

# êµ¬ê¸€ ì œë¯¸ë‚˜ì´ API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ í‚¤ë¥¼ ê°€ì ¸ì˜´)
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("[WARN] GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš”ì•½ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ============== ìœ í‹¸ ==============
def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(
        total=3,
        backoff_factor=0.3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"],
        raise_on_status=False,
    )
    ad = HTTPAdapter(max_retries=retries)
    s.mount("http://", ad)
    s.mount("https://", ad)
    s.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NewsCrawler/1.0)"})
    return s

def normalize_url(url: str) -> str:
    """êµ¬ê¸€ë‰´ìŠ¤ ë§í¬ ì •ê·œí™”"""
    try:
        p = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(p.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}:
                continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        nq = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit((p.scheme.lower(), p.netloc.lower(), p.path, nq, ""))
    except Exception:
        return url

def extract_domain(url: str) -> str:
    try:
        return urllib.parse.urlsplit(url).netloc.lower()
    except Exception:
        return ""

def parse_pub_date(text: str):
    if not text:
        return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")

def utc_to_kst_str(utc_ts):
    if utc_ts is None or pd.isna(utc_ts):
        return ""
    try:
        ts = pd.to_datetime(utc_ts, utc=True, errors="coerce")
        if pd.isna(ts):
            return ""
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except Exception:
        return ""

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

def resolve_final_url(session: requests.Session, url: str, timeout: float = 8.0) -> str:
    try:
        r = session.head(url, allow_redirects=True, timeout=timeout)
        fu = r.url
        if not fu or fu == url:
            r = session.get(url, allow_redirects=True, timeout=timeout)
            fu = r.url
        return fu or url
    except Exception:
        return url

# ============== AI ë° ë³¸ë¬¸ ì¶”ì¶œ ==============
def extract_article_content(url: str) -> str:
    """newspaper3kë¥¼ ì´ìš©í•´ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        text = article.text.strip()
        if len(text) < 50: # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
            return ""
        return text
    except Exception:
        return ""

def summarize_with_gemini(text: str) -> str:
    """Google Gemini Flash ëª¨ë¸ì„ ì´ìš©í•œ 3ì¤„ ìš”ì•½"""
    if not GEMINI_API_KEY:
        return "API Key ë¯¸ì„¤ì •"
    if not text:
        return "ë³¸ë¬¸ ì—†ìŒ"
    
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = (
            "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ ë¹„ì„œì•¼. ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ì¶”ë ¤ì„œ "
            "3ê°œì˜ ë¶ˆë › í¬ì¸íŠ¸(- )ë¡œ ìš”ì•½í•´ì¤˜. ì–´ì¡°ëŠ” ê±´ì¡°í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ í•´ì¤˜.\n\n"
            f"ê¸°ì‚¬ ë‚´ìš©:\n{text[:5000]}" # í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ê¸¸ì´ ì ˆì‚­
        )
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"ìš”ì•½ ì‹¤íŒ¨"

# ============== í¬ë¡¤ë§ ==============
def crawl_google_news_rss(session: requests.Session, keyword: str):
    q = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    try:
        resp = session.get(url, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ERROR] RSS ì ‘ì† ì‹¤íŒ¨ ({keyword}): {e}")
        return []

    soup = BeautifulSoup(resp.text, "xml")
    items = soup.find_all("item")
    collected_at_utc = pd.Timestamp.now(tz="UTC")

    rows = []
    for it in items:
        title = it.title.text if it.title else ""
        link = it.link.text if it.link else ""
        pub_ts_utc = parse_pub_date(it.pubDate.text if it.pubDate else "")
        
        # 1ì°¨ì ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ í•´ì†Œ (ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•´ í•„ìˆ˜)
        final_link = resolve_final_url(session, link)

        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": pub_ts_utc,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at_utc,
            "_ì •ê·œí™”ë§í¬": normalize_url(final_link), # ì •ê·œí™” ê¸°ì¤€ì„ final_linkë¡œ ë³€ê²½
            "ìš”ì•½": "" # ì´ˆê¸°ì—” ë¹ˆ ê°’
        })
    print(f"âœ… '{keyword}' RSS {len(rows)}ê±´ í™•ì¸")
    return rows

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    # 1) ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    all_path = DATA_DIR / "ALL.csv"
    required_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜","ìš”ì•½",
                     "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"]
    
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        # ì»¬ëŸ¼ ë³´ì •
        for col in required_cols:
            if col not in df_existing.columns:
                df_existing[col] = ""
        # ê¸°ì¡´ ë°ì´í„°ì˜ ë§í¬ ì§‘í•© (ì¤‘ë³µ í™•ì¸ìš©)
        existing_links = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str))
    else:
        df_existing = pd.DataFrame(columns=required_cols)
        existing_links = set()

    # 2) ì‹ ê·œ ìˆ˜ì§‘ (ì¼ë‹¨ ê¸ì–´ì˜´)
    raw_rows = []
    for kw in KEYWORDS:
        raw_rows.extend(crawl_google_news_rss(session, kw))
        time.sleep(0.5)
    
    if not raw_rows:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df_crawled = pd.DataFrame(raw_rows)

    # 3) ì¤‘ë³µ ì œê±° í›„ 'ì§„ì§œ ì‹ ê·œ' ì‹ë³„
    # ì •ê·œí™” ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ì— ì—†ëŠ” ê²ƒë§Œ í•„í„°ë§
    df_crawled["_is_new"] = ~df_crawled["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_links)
    
    # ì¤‘ë³µ ì œê±° (ì´ë²ˆ ìˆ˜ì§‘ ë‚´ì—ì„œ ì¤‘ë³µ ë°©ì§€)
    df_crawled = df_crawled.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
    
    # ì§„ì§œ ì²˜ë¦¬í•´ì•¼ í•  ì‹ ê·œ ë°ì´í„°
    df_to_process = df_crawled[df_crawled["_is_new"] == True].copy()
    
    print(f"ğŸ” ì „ì²´ {len(df_crawled)}ê±´ ì¤‘ ì‹ ê·œ ê¸°ì‚¬ {len(df_to_process)}ê±´ ë°œê²¬. ìš”ì•½ ì‹œì‘...")

    # 4) ì‹ ê·œ ë°ì´í„°ì— ëŒ€í•´ ë³¸ë¬¸ ì¶”ì¶œ & ìš”ì•½ ìˆ˜í–‰
    processed_rows = []
    # DataFrame ìˆœíšŒ ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²˜ë¦¬ (ì†ë„/ì•ˆì „ì„±)
    for idx, row in df_to_process.iterrows():
        url = row["ì›ë¬¸ë§í¬"]
        print(f"   Processing: {row['ì œëª©'][:20]}...")
        
        # A. ë³¸ë¬¸ ì¶”ì¶œ
        content = extract_article_content(url)
        
        # B. ìš”ì•½ (ë³¸ë¬¸ì´ ìˆì„ ë•Œë§Œ)
        summary = ""
        if content:
            summary = summarize_with_gemini(content)
            # ë¬´ë£Œ í‹°ì–´ ì†ë„ ì œí•œ ê³ ë ¤ (ë¶„ë‹¹ 15íšŒ ë“±) -> ì•ˆì „í•˜ê²Œ 4ì´ˆ ëŒ€ê¸°
            time.sleep(4) 
        else:
            summary = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        
        row["ìš”ì•½"] = summary
        processed_rows.append(row)

    # ì²˜ë¦¬ëœ ì‹ ê·œ ë°ì´í„° DF ìƒì„±
    if processed_rows:
        df_new_processed = pd.DataFrame(processed_rows)
    else:
        df_new_processed = pd.DataFrame(columns=df_crawled.columns)

    # 5) ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© ì „ í¬ë§·íŒ…
    # ë‚ ì§œ ë³€í™˜
    if not df_new_processed.empty:
        df_new_processed["ë°œí–‰ì¼(KST)"] = df_new_processed["ë°œí–‰ì¼_UTC"].apply(utc_to_kst_str)
        df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new_processed["ìˆ˜ì§‘ì‹œê°_UTC"].apply(utc_to_kst_str)
        df_new_processed["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new_processed["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new_processed["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ìŠ¤í‚¤ë§ˆ ë§ì¶”ê¸° (ì´ë¯¸ ìˆëŠ” ë°ì´í„°ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    df_final_new = df_new_processed[required_cols] if not df_new_processed.empty else pd.DataFrame(columns=required_cols)
    
    # ë³‘í•©
    combined = pd.concat([df_existing, df_final_new], ignore_index=True)
    
    # ìµœì¢… ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="last") # ìµœì‹  ì •ë³´(ìš”ì•½í¬í•¨) ìš°ì„ 
    combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False)

    # 6) ì €ì¥
    # í‘œì‹œìš© ì»¬ëŸ¼ ì •ì˜
    display_cols = ["í‚¤ì›Œë“œ","ì œëª©","ìš”ì•½","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    
    # ë©”ì¸ íŒŒì¼ ì €ì¥
    combined[display_cols].to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    
    # í‚¤ì›Œë“œë³„ íŒŒì¼ ì €ì¥
    for kw, g in combined.groupby("í‚¤ì›Œë“œ"):
        g[display_cols].to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")
        
    # ì´ë²ˆ ì‹¤í–‰ì˜ ì‹ ê·œ íŒŒì¼ ì €ì¥
    if not df_new_processed.empty:
        df_new_processed[display_cols].to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ ì‹ ê·œ {len(df_new_processed)}ê±´ ìš”ì•½ ë° ì €ì¥ ì™„ë£Œ")
    else:
        # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ì—ëŸ¬ ë°©ì§€
        pd.DataFrame(columns=display_cols).to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
        print("ğŸ‰ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)")

if __name__ == "__main__":
    main()
