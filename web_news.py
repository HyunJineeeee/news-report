# web_news.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai
from newspaper import Article, Config
import base64

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨", "ê³ ìš©ë…¸ë™ë¶€", "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨"]
DATA_DIR = Path("data")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ============== ìœ í‹¸ ==============
def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(total=3, backoff_factor=0.3, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["GET", "HEAD"], raise_on_status=False)
    ad = HTTPAdapter(max_retries=retries)
    s.mount("http://", ad)
    s.mount("https://", ad)
    # êµ¬ê¸€ ë´‡ ì°¨ë‹¨ ìš°íšŒìš© í—¤ë” ë° ì¿ í‚¤
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
    })
    # ì¿ í‚¤ ì¶”ê°€ (Consent í˜ì´ì§€ ìš°íšŒ ì‹œë„)
    s.cookies.set("CONSENT", "YES+KR.ko+V14+BX")
    return s

def normalize_url(url: str) -> str:
    try:
        p = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(p.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}: continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        nq = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit((p.scheme.lower(), p.netloc.lower(), p.path, nq, ""))
    except: return url

def extract_domain(url: str) -> str:
    try: return urllib.parse.urlsplit(url).netloc.lower()
    except: return ""

def parse_pub_date(text: str):
    if not text: return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")

def utc_to_kst_str(utc_ts):
    if utc_ts is None or pd.isna(utc_ts): return ""
    try:
        ts = pd.to_datetime(utc_ts, utc=True, errors="coerce")
        if pd.isna(ts): return ""
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except: return ""

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

def get_real_url(session, url):
    """
    êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì„ ì¶”ì í•˜ì—¬ ì§„ì§œ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ë‹¨ìˆœ requestsë¿ë§Œ ì•„ë‹ˆë¼ base64 ë””ì½”ë”© ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ ì›ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    if "news.google.com" not in url:
        return url
    
    try:
        # 1ì°¨ ì‹œë„: í—¤ë”ì™€ ì¿ í‚¤ë¥¼ ë‹¬ê³  ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì 
        r = session.get(url, allow_redirects=True, timeout=5)
        if "news.google.com" not in r.url:
            return r.url
            
        # 2ì°¨ ì‹œë„: êµ¬ê¸€ ë‰´ìŠ¤ URL êµ¬ì¡°ìƒ base64ë¡œ ì¸ì½”ë”©ëœ ë¶€ë¶„ì´ ìˆì„ ìˆ˜ ìˆìŒ (ë‹¨ìˆœí™”ëœ ë¡œì§)
        # (êµ¬ê¸€ì˜ ì•”í˜¸í™” ë°©ì‹ì€ ë³µì¡í•˜ì—¬ ì™„ë²½í•œ ë””ì½”ë”©ì€ ì–´ë ¤ìš°ë‚˜, ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ HTML ë‚´ì—ì„œ ì°¾ëŠ” ë°©ì‹ ì‚¬ìš©)
        soup = BeautifulSoup(r.text, "html.parser")
        # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ í˜ì´ì§€ì˜ <a> íƒœê·¸ë‚˜ <c-wiz> ë“±ì„ ë’¤ì ¸ë´„
        links = soup.find_all("a", href=True)
        for link in links:
            href = link['href']
            # êµ¬ê¸€ ë‚´ë¶€ ë§í¬ê°€ ì•„ë‹ˆê³  httpë¡œ ì‹œì‘í•˜ë©´ ì›ë¬¸ì¼ í™•ë¥  ë†’ìŒ
            if href.startswith("http") and "google.com" not in href:
                return href
                
        return r.url # ì‹¤íŒ¨í•˜ë©´ ì›ë˜ URL ë°˜í™˜
    except:
        return url

# ============== AI & ë³¸ë¬¸ ì¶”ì¶œ ==============
def extract_article_content(url: str) -> str:
    try:
        # ì—¬ì „íˆ êµ¬ê¸€ ë§í¬ë¼ë©´ newspaper3këŠ” ì‹¤íŒ¨í•¨
        if "news.google.com" in url:
            return ""

        config = Config()
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© User-Agent
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10

        article = Article(url, language='ko', config=config)
        article.download()
        article.parse()
        text = article.text.strip()
        
        return text if len(text) >= 100 else "" 
    except Exception as e:
        return ""

def summarize_with_gemini(text: str) -> str:
    if not GEMINI_API_KEY or not text: return ""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = (
            "ë„ˆëŠ” ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ì½ê³ , "
            "ë°”ìœ ì§ì¥ì¸ì´ í•µì‹¬ë§Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜.\n"
            "í˜•ì‹: '- 'ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥.\n\n"
            f"ê¸°ì‚¬ ë‚´ìš©:\n{text[:4000]}"
        )
        response = model.generate_content(prompt)
        return response.text.strip()
    except: return ""

# ============== ì´ë©”ì¼ ë°œì†¡ (ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ ì ìš©) ==============
def send_email_report(df_new, target_date_str):
    if not EMAIL_USER or not EMAIL_PASSWORD or not EMAIL_RECEIVER:
        print("[WARN] ì´ë©”ì¼ ì„¤ì • ëˆ„ë½. ë°œì†¡ ìƒëµ.")
        return
    
    if df_new.empty: 
        print("ğŸ“­ ë°œì†¡í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    subject = f"[ì¼ë³‘ë¦¬í¬íŠ¸] {target_date_str} ì£¼ìš” ë‰´ìŠ¤ ì•Œë¦¼"

    # â˜…â˜…â˜… ì¸ë¼ì¸ ìŠ¤íƒ€ì¼(Inline Style) ì ìš© â˜…â˜…â˜…
    # <style> íƒœê·¸ë¥¼ ì“°ì§€ ì•Šê³ , íƒœê·¸ ì•ˆì— style="..."ì„ ì§ì ‘ ë„£ì–´ì„œ ëª¨ë“  ì´ë©”ì¼ í´ë¼ì´ì–¸íŠ¸ í˜¸í™˜ì„± í™•ë³´
    
    html_body = f"""
    <div style="font-family: 'Malgun Gothic', sans-serif; background-color: #f4f4f4; padding: 20px; color: #333;">
        <div style="max-width: 700px; margin: 0 auto; background-color: #ffffff; padding: 30px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.05);">
            
            <div style="text-align: center; margin-bottom: 30px; border-bottom: 2px solid #3498db; padding-bottom: 20px;">
                <h1 style="color: #2c3e50; font-size: 24px; margin: 0;">ğŸ“° {target_date_str} ë‰´ìŠ¤ ë¦¬í¬íŠ¸</h1>
                <p style="color: #7f8c8d; font-size: 14px; margin-top: 10px;">
                    ì–´ì œ ìˆ˜ì§‘ëœ ì´ <span style="color:#e67e22; font-weight:bold;">{len(df_new)}</span>ê±´ì˜ ê¸°ì‚¬ ìš”ì•½ì…ë‹ˆë‹¤.
                </p>
            </div>
    """

    grouped = df_new.groupby("í‚¤ì›Œë“œ")
    
    for kw in KEYWORDS:
        if kw in grouped.groups:
            group_df = grouped.get_group(kw)
            
            # í‚¤ì›Œë“œ ì œëª©
            html_body += f"""
            <div style="margin-bottom: 30px;">
                <div style="background-color: #3498db; color: white; padding: 6px 15px; display: inline-block; border-radius: 15px; font-weight: bold; font-size: 16px; margin-bottom: 15px;">
                    # {kw}
                </div>
            """
            
            for idx, row in group_df.iterrows():
                title = row['ì œëª©']
                link = row['ì›ë¬¸ë§í¬']
                source = row['ì¶œì²˜']
                date = row['ë°œí–‰ì¼(KST)']
                summary = row['ìš”ì•½']

                # ìš”ì•½ HTML ì²˜ë¦¬
                if summary:
                    summary_html = summary.replace('\n', '<br>')
                    summary_style = "background-color: #f9f9f9; padding: 15px; border-left: 4px solid #3498db; color: #555; font-size: 14px; line-height: 1.6; border-radius: 4px;"
                else:
                    summary_html = "ğŸ‘‰ í´ë¦­í•˜ì—¬ ì›ë¬¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”."
                    summary_style = "background-color: #f0f0f0; padding: 10px; color: #888; font-size: 13px; text-align: center; border-radius: 4px;"

                # ê¸°ì‚¬ ì¹´ë“œ
                html_body += f"""
                <div style="border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 15px; background-color: #fff;">
                    <a href="{link}" target="_blank" style="font-size: 18px; font-weight: bold; color: #2c3e50; text-decoration: none; display: block; margin-bottom: 8px; line-height: 1.4;">
                        {title}
                    </a>
                    
                    <div style="font-size: 12px; color: #95a5a6; margin-bottom: 15px;">
                        {source} | {date}
                    </div>
                    
                    <div style="{summary_style}">
                        {summary_html}
                    </div>
                    
                    <div style="text-align: right; margin-top: 10px;">
                        <a href="{link}" target="_blank" style="display: inline-block; background-color: #ecf0f1; color: #555; padding: 5px 12px; border-radius: 4px; text-decoration: none; font-size: 12px;">
                            ì›ë¬¸ ë³´ëŸ¬ê°€ê¸° â†’
                        </a>
                    </div>
                </div>
                """
            html_body += '</div>' # í‚¤ì›Œë“œ ì„¹ì…˜ ë‹«ê¸°

    html_body += """
            <div style="text-align: center; margin-top: 40px; font-size: 12px; color: #bdc3c7; border-top: 1px solid #eee; padding-top: 20px;">
                Automated by GitHub Actions & Google Gemini
            </div>
        </div>
    </div>
    """

    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_RECEIVER
        msg.attach(MIMEText(html_body, 'html'))

        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(EMAIL_USER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! ({subject})")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

# ============== í¬ë¡¤ë§ ==============
def crawl_google_news_rss(session, keyword):
    q = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={q}+when:1d&hl=ko&gl=KR&ceid=KR:ko"
    try:
        resp = session.get(url, timeout=20)
        resp.raise_for_status()
    except: return []

    soup = BeautifulSoup(resp.text, "xml")
    rows = []
    collected_at_utc = pd.Timestamp.now(tz="UTC")
    
    for it in soup.find_all("item"):
        title = it.title.text if it.title else ""
        link = it.link.text if it.link else ""
        pub_date_str = it.pubDate.text if it.pubDate else ""
        pub_ts_utc = parse_pub_date(pub_date_str)
        
        # ì§„ì§œ URL ì¶”ì  ì‹œë„
        final_link = get_real_url(session, link)
        
        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": pub_ts_utc,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at_utc,
            "_ì •ê·œí™”ë§í¬": normalize_url(final_link),
            "ìš”ì•½": ""
        })
    return rows

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    # íƒ€ê²Ÿ ë‚ ì§œ: "ì–´ì œ"
    now_kst = pd.Timestamp.now(tz="Asia/Seoul")
    yesterday_kst = now_kst - pd.Timedelta(days=1)
    target_date_str = yesterday_kst.strftime("%Y-%m-%d")
    print(f"ğŸ¯ íƒ€ê²Ÿ ë‚ ì§œ(ì–´ì œ): {target_date_str}")

    all_path = DATA_DIR / "ALL.csv"
    req_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜","ìš”ì•½",
                "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"]
    
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        for c in req_cols: 
            if c not in df_existing.columns: df_existing[c] = ""
        df_existing["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_existing["_ìˆ˜ì§‘ì‹œê°_dt"], errors="coerce")
        existing_links = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str))
    else:
        df_existing = pd.DataFrame(columns=req_cols)
        existing_links = set()

    # í¬ë¡¤ë§
    raw_rows = []
    for kw in KEYWORDS:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘: {kw}...")
        raw_rows.extend(crawl_google_news_rss(session, kw))
        time.sleep(1)
    
    if not raw_rows: 
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df_crawled = pd.DataFrame(raw_rows)
    
    # ì–´ì œ ë‚ ì§œ í•„í„°ë§
    df_crawled["ë°œí–‰ì¼(KST)"] = df_crawled["ë°œí–‰ì¼_UTC"].apply(utc_to_kst_str)
    df_crawled = df_crawled[df_crawled["ë°œí–‰ì¼(KST)"].str.startswith(target_date_str)]
    
    if df_crawled.empty:
        print(f"ğŸ“… {target_date_str} ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¤‘ë³µ ì œê±°
    df_crawled["_is_new"] = ~df_crawled["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_links)
    df_crawled = df_crawled.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
    
    df_to_process = df_crawled[df_crawled["_is_new"] == True].copy()
    print(f"ğŸ” {target_date_str} ê¸°ì‚¬ ì¤‘ ì‹ ê·œ {len(df_to_process)}ê±´ ë°œê²¬.")

    # ìš”ì•½
    processed_rows = []
    for idx, row in df_to_process.iterrows():
        print(f"   Processing: {row['ì œëª©'][:20]}...")
        real_url = row["ì›ë¬¸ë§í¬"]
        
        # ìš”ì•½ ì‹œë„
        content = extract_article_content(real_url)
        summary = ""
        if content:
            summary = summarize_with_gemini(content)
            time.sleep(2) 
            
        row["ìš”ì•½"] = summary
        processed_rows.append(row)

    if processed_rows:
        df_new_processed = pd.DataFrame(processed_rows)
        # ë©”ì¼ ë°œì†¡
        df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new_processed["ìˆ˜ì§‘ì‹œê°_UTC"].apply(utc_to_kst_str)
        df_new_processed["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new_processed["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new_processed["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        send_email_report(df_new_processed, target_date_str)
    else:
        df_new_processed = pd.DataFrame(columns=df_crawled.columns)

    # ì €ì¥
    df_final_new = df_new_processed[req_cols] if not df_new_processed.empty else pd.DataFrame(columns=req_cols)
    combined = pd.concat([df_existing, df_final_new], ignore_index=True)
    combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="last")
    combined["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(combined["_ìˆ˜ì§‘ì‹œê°_dt"], errors="coerce")
    combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False)

    display_cols = ["í‚¤ì›Œë“œ","ì œëª©","ìš”ì•½","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    combined[display_cols].to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    
    if not df_new_processed.empty:
        df_new_processed[display_cols].to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
    
    print("ğŸ‰ ì™„ë£Œ")

if __name__ == "__main__":
    main()
