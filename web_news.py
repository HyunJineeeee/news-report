# web_news.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai
from newspaper import Article

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨", "ê³ ìš©ë…¸ë™ë¶€", "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨"]
DATA_DIR = Path("data")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ============== ìœ í‹¸ ==============
def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(total=3, backoff_factor=0.3, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["GET", "HEAD"], raise_on_status=False)
    ad = HTTPAdapter(max_retries=retries)
    s.mount("http://", ad)
    s.mount("https://", ad)
    s.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"})
    return s

def normalize_url(url: str) -> str:
    try:
        p = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(p.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}: continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        nq = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit((p.scheme.lower(), p.netloc.lower(), p.path, nq, ""))
    except: return url

def extract_domain(url: str) -> str:
    try: return urllib.parse.urlsplit(url).netloc.lower()
    except: return ""

def parse_pub_date(text: str):
    if not text: return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")

def utc_to_kst_str(utc_ts):
    if utc_ts is None or pd.isna(utc_ts): return ""
    try:
        ts = pd.to_datetime(utc_ts, utc=True, errors="coerce")
        if pd.isna(ts): return ""
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except: return ""

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

def resolve_final_url(session: requests.Session, url: str, timeout: float = 10.0) -> str:
    try:
        # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ë´‡ì„ ê°ì§€í•˜ë©´ ì¤‘ê°„ í˜ì´ì§€ì—ì„œ ë©ˆì¶¤. 
        # ìµœëŒ€í•œ ë¸Œë¼ìš°ì €ì¸ì²™ í—¤ë”ë¥¼ ë„£ì–´ì„œ ì‹œë„
        r = session.get(url, allow_redirects=True, timeout=timeout)
        return r.url
    except: return url

# ============== AI & ë³¸ë¬¸ ì¶”ì¶œ ==============
def extract_article_content(url: str) -> str:
    try:
        # User-Agent ì„¤ì •ì´ ëœ config ê°ì²´ë¥¼ ì‚¬ìš©í•˜ë©´ ì„±ê³µë¥ ì´ ì¡°ê¸ˆ ì˜¤ë¦„
        article = Article(url, language='ko')
        article.download()
        article.parse()
        text = article.text.strip()
        return text if len(text) >= 50 else ""
    except: return ""

def summarize_with_gemini(text: str) -> str:
    if not GEMINI_API_KEY or not text: return ""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = (
            "ë„ˆëŠ” ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ë¹„ì„œì•¼. ì•„ë˜ ê¸°ì‚¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 2~3ì¤„ë¡œ ìš”ì•½í•´ì¤˜.\n"
            "ë‹¨, ê¸°ì‚¬ ì œëª©ì— ìˆëŠ” ë‚´ìš©ì„ ë‹¨ìˆœíˆ ë°˜ë³µí•˜ì§€ ë§ê³ , ì œëª©ì´ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” 'êµ¬ì²´ì ì¸ ìˆ˜ì¹˜', 'ë°°ê²½', 'í–¥í›„ ê³„íš' ìœ„ì£¼ë¡œ ìš”ì•½í•´.\n"
            "ë¬¸ì¥ì€ '- 'ë¡œ ì‹œì‘í•˜ëŠ” ê°œì¡°ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.\n\n"
            f"ê¸°ì‚¬ ë‚´ìš©:\n{text[:5000]}"
        )
        response = model.generate_content(prompt)
        return response.text.strip()
    except: return ""

def clean_html_tags(text: str) -> str:
    """RSS descriptionì— ìˆëŠ” HTML íƒœê·¸ ì œê±°"""
    return re.sub(r'<[^>]+>', '', text).strip()

# ============== ì´ë©”ì¼ ë°œì†¡ ==============
def send_email_report(df_new):
    if not EMAIL_USER or not EMAIL_PASSWORD or not EMAIL_RECEIVER:
        print("[WARN] ì´ë©”ì¼ ì„¤ì • ëˆ„ë½. ë°œì†¡ ìƒëµ.")
        return
    
    if df_new.empty: return

    today_str = pd.Timestamp.now(tz="Asia/Seoul").strftime('%Y-%m-%d')
    subject = f"[ì¼ë³‘ë¦¬í¬íŠ¸] {today_str} ì‹ ê·œê¸°ì‚¬ ì•Œë¦¼"

    html_body = f"""
    <html>
    <head>
        <style>
            body {{ font-family: 'Apple SD Gothic Neo', 'Malgun Gothic', Arial, sans-serif; line-height: 1.6; color: #333; }}
            .container {{ max-width: 800px; margin: 0 auto; padding: 20px; }}
            .header {{ background-color: #f4f6f8; padding: 15px; border-radius: 8px; margin-bottom: 20px; }}
            .keyword-group {{ margin-bottom: 30px; border-bottom: 2px solid #eee; padding-bottom: 20px; }}
            .keyword-title {{ color: #2980b9; font-size: 18px; font-weight: bold; margin-bottom: 15px; border-left: 5px solid #2980b9; padding-left: 10px; }}
            .news-item {{ margin-bottom: 15px; }}
            .news-title {{ font-size: 15px; font-weight: bold; color: #2c3e50; text-decoration: none; }}
            .news-title:hover {{ text-decoration: underline; }}
            .news-meta {{ font-size: 12px; color: #7f8c8d; margin-left: 5px; }}
            .news-summary {{ margin-top: 5px; margin-left: 15px; font-size: 13px; color: #555; background-color: #fafafa; padding: 8px; border-radius: 4px; }}
            .footer {{ font-size: 11px; color: #aaa; text-align: center; margin-top: 30px; }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h2 style="margin:0;">ğŸ“¢ ì˜¤ëŠ˜ì˜ ì§ì—…í›ˆë ¨ ë‰´ìŠ¤ ë¦¬í¬íŠ¸</h2>
                <p style="margin:5px 0 0 0; font-size:14px; color:#666;">
                    ì´ {len(df_new)}ê±´ì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.
                </p>
            </div>
    """

    grouped = df_new.groupby("í‚¤ì›Œë“œ")
    
    for kw in KEYWORDS:
        if kw in grouped.groups:
            group_df = grouped.get_group(kw)
            html_body += f'<div class="keyword-group">'
            html_body += f'<div class="keyword-title">ğŸ“ƒ í‚¤ì›Œë“œ: {kw}</div>'
            
            for i, (_, row) in enumerate(group_df.iterrows(), 1):
                title = row['ì œëª©']
                link = row['ì›ë¬¸ë§í¬']
                source = row['ì¶œì²˜']
                date = row['ë°œí–‰ì¼(KST)']
                summary = row['ìš”ì•½']

                # ìš”ì•½ì´ ë¹„ì–´ìˆìœ¼ë©´ í‘œì‹œí•˜ì§€ ì•Šê±°ë‚˜ ì•ˆë‚´ ë¬¸êµ¬
                if not summary:
                    summary_html = "<span style='color:#ccc; font-size:12px;'>(ìš”ì•½ ì—†ìŒ)</span>"
                else:
                    summary_html = summary.replace('\n', '<br>')

                html_body += f"""
                <div class="news-item">
                    <div>
                        <span style="color:#e67e22; font-weight:bold; margin-right:5px;">{i}.</span>
                        <a href="{link}" class="news-title" target="_blank">{title}</a>
                        <span class="news-meta">({source} | {date})</span>
                    </div>
                    <div class="news-summary">
                        {summary_html}
                    </div>
                </div>
                """
            html_body += '</div>'

    html_body += """
            <div class="footer">
                ë³¸ ë©”ì¼ì€ ìë™í™” ë´‡ì— ì˜í•´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.<br>
                GitHub Actions & Google Gemini API
            </div>
        </div>
    </body>
    </html>
    """

    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_RECEIVER
        msg.attach(MIMEText(html_body, 'html'))

        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(EMAIL_USER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! ({subject})")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

# ============== í¬ë¡¤ë§ ==============
def crawl_google_news_rss(session, keyword):
    q = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    try:
        resp = session.get(url, timeout=20)
        resp.raise_for_status()
    except: return []

    soup = BeautifulSoup(resp.text, "xml")
    rows = []
    collected_at_utc = pd.Timestamp.now(tz="UTC")
    
    for it in soup.find_all("item"):
        title = it.title.text if it.title else ""
        link = it.link.text if it.link else ""
        # RSSì— í¬í•¨ëœ ê¸°ë³¸ ì„¤ëª…ê¸€ (HTML íƒœê·¸ í¬í•¨ë¨)
        description = it.description.text if it.description else ""
        
        final_link = resolve_final_url(session, link)
        
        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": parse_pub_date(it.pubDate.text if it.pubDate else ""),
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at_utc,
            "_ì •ê·œí™”ë§í¬": normalize_url(final_link),
            "ìš”ì•½": "", # ë‚˜ì¤‘ì— ì±„ì›€
            "_rss_desc": clean_html_tags(description) # ë°±ì—…ìš© RSS ì„¤ëª… ì €ì¥
        })
    print(f"âœ… '{keyword}' {len(rows)}ê±´")
    return rows

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    all_path = DATA_DIR / "ALL.csv"
    req_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜","ìš”ì•½",
                "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"]
    
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        for c in req_cols: 
            if c not in df_existing.columns: df_existing[c] = ""
        existing_links = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str))
    else:
        df_existing = pd.DataFrame(columns=req_cols)
        existing_links = set()

    raw_rows = []
    for kw in KEYWORDS:
        raw_rows.extend(crawl_google_news_rss(session, kw))
        time.sleep(0.5)
    
    if not raw_rows: 
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df_crawled = pd.DataFrame(raw_rows)
    df_crawled["_is_new"] = ~df_crawled["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_links)
    df_crawled = df_crawled.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
    
    df_to_process = df_crawled[df_crawled["_is_new"] == True].copy()
    print(f"ğŸ” ì‹ ê·œ {len(df_to_process)}ê±´ ë°œê²¬.")

    processed_rows = []
    for idx, row in df_to_process.iterrows():
        print(f"   Processing: {row['ì œëª©'][:20]}...")
        content = extract_article_content(row["ì›ë¬¸ë§í¬"])
        
        summary = ""
        # 1. ë³¸ë¬¸ì´ ìˆìœ¼ë©´ AI ìš”ì•½ ì‹œë„
        if content:
            ai_summary = summarize_with_gemini(content)
            if ai_summary and "ì‹¤íŒ¨" not in ai_summary:
                summary = ai_summary
                time.sleep(4)
        
        # 2. ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨í–ˆê±°ë‚˜ AI ìš”ì•½ ì‹¤íŒ¨ì‹œ -> RSS ê¸°ë³¸ ì„¤ëª…(description) ì‚¬ìš©
        if not summary or summary == "ìš”ì•½ ìƒì„± ì‹¤íŒ¨":
            # RSS ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(ì œëª©ê³¼ ê°™ìœ¼ë©´) ê·¸ëƒ¥ "ë‚´ìš© í™•ì¸" ë¬¸êµ¬ë¡œ ëŒ€ì²´
            rss_desc = row.get("_rss_desc", "")
            if len(rss_desc) > 10 and rss_desc != row['ì œëª©']:
                summary = f"- (AI ìš”ì•½ ë¶ˆê°€) {rss_desc[:150]}..."
            else:
                summary = "ğŸ‘‰ ì›ë¬¸ ë§í¬ì—ì„œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”."

        row["ìš”ì•½"] = summary
        processed_rows.append(row)

    if processed_rows:
        df_new_processed = pd.DataFrame(processed_rows)
    else:
        df_new_processed = pd.DataFrame(columns=df_crawled.columns)

    if not df_new_processed.empty:
        df_new_processed["ë°œí–‰ì¼(KST)"] = df_new_processed["ë°œí–‰ì¼_UTC"].apply(utc_to_kst_str)
        df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new_processed["ìˆ˜ì§‘ì‹œê°_UTC"].apply(utc_to_kst_str)
        df_new_processed["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new_processed["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new_processed["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new_processed["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        
        send_email_report(df_new_processed)

    # ì €ì¥ ì‹œ _rss_desc ì»¬ëŸ¼ì€ ì œì™¸
    df_final_new = df_new_processed[req_cols] if not df_new_processed.empty else pd.DataFrame(columns=req_cols)
    combined = pd.concat([df_existing, df_final_new], ignore_index=True)
    combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="last").sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False)

    display_cols = ["í‚¤ì›Œë“œ","ì œëª©","ìš”ì•½","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    combined[display_cols].to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    for kw, g in combined.groupby("í‚¤ì›Œë“œ"):
        g[display_cols].to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")
    
    if not df_new_processed.empty:
        df_new_processed[display_cols].to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
    else:
        pd.DataFrame(columns=display_cols).to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
    
    print("ğŸ‰ ì™„ë£Œ")

if __name__ == "__main__":
    main()
