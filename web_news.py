import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime
import re

# === ì„¤ì • ===
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨"]
DATA_DIR = Path("data")
LOG_DIR = Path("logs")
WRITE_LOG_FILE = True

# === ìœ í‹¸ ===
def make_session() -> requests.Session:
    sess = requests.Session()
    retries = Retry(total=5, backoff_factor=0.5,
                    status_forcelist=[429, 500, 502, 503, 504],
                    allowed_methods=["GET"], raise_on_status=False)
    adapter = HTTPAdapter(max_retries=retries)
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    sess.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NewsCrawler/1.0)"})
    return sess

def normalize_url(url: str) -> str:
    try:
        parsed = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)
        cleaned = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl","gl","ceid","oc"}: 
                continue
            cleaned.append((k, v))
        cleaned.sort(key=lambda x: x[0])
        new_query = urllib.parse.urlencode(cleaned, doseq=True)
        return urllib.parse.urlunsplit(
            (parsed.scheme.lower(), parsed.netloc.lower(), parsed.path, new_query, "")
        )
    except Exception:
        return url

def extract_domain(url: str) -> str:
    try:
        return urllib.parse.urlsplit(url).netloc.lower()
    except Exception:
        return ""

def parse_pub_date(text: str):
    if not text: 
        return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")

def to_kst_str_from_utc(ts):
    if pd.isna(ts):
        return None
    try:
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except Exception:
        return None

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

# === í¬ë¡¤ëŸ¬ ===
def crawl_google_news_rss(session: requests.Session, keyword: str):
    encoded_kw = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=ko&gl=KR&ceid=KR:ko"
    resp = session.get(url, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "xml")
    items = soup.find_all("item")

    collected_at = pd.Timestamp.utcnow()
    rows = []
    for item in items:
        title = item.title.text if item.title else ""
        link = item.link.text if item.link else ""
        pub_date_raw = item.pubDate.text if item.pubDate else ""
        pub_ts = parse_pub_date(pub_date_raw)
        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ë§í¬": link,
            "ë°œí–‰ì¼_UTC": pub_ts,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at,
            "_ì •ê·œí™”ë§í¬": normalize_url(link),
            "ì¶œì²˜": extract_domain(link),
        })
    print(f"âœ… '{keyword}' ë‰´ìŠ¤ {len(rows)}ê±´ ìˆ˜ì§‘")
    return rows

# === ë©”ì¸ ===
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    # ê¸°ì¡´ ALL.csv ë¶ˆëŸ¬ì˜¤ê¸°(ëˆ„ì )
    all_path = DATA_DIR / "ALL.csv"
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        for c in ["ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)"]:
            df_existing[c] = pd.to_datetime(df_existing[c], errors="coerce")
        df_existing["_ì •ê·œí™”ë§í¬"] = df_existing["ë§í¬"].fillna("").apply(normalize_url)
        df_existing["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_existing["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_existing["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_existing["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        df_existing["_is_new"] = False
    else:
        df_existing = pd.DataFrame(columns=[
            "í‚¤ì›Œë“œ","ì œëª©","ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜",
            "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"
        ])

    # ì‹ ê·œ ìˆ˜ì§‘
    all_rows = []
    for kw in KEYWORDS:
        all_rows.extend(crawl_google_news_rss(session, kw))
        time.sleep(0.5)

    df_new_raw = pd.DataFrame(all_rows)
    if not df_new_raw.empty:
        df_new_raw["ë°œí–‰ì¼(KST)"] = df_new_raw["ë°œí–‰ì¼_UTC"].apply(to_kst_str_from_utc)
        df_new_raw["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new_raw["ìˆ˜ì§‘ì‹œê°_UTC"].apply(to_kst_str_from_utc)
        df_new_raw["_ì •ê·œí™”ë§í¬"] = df_new_raw["_ì •ê·œí™”ë§í¬"].fillna(df_new_raw["ë§í¬"]).apply(normalize_url)
        df_new_raw["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new_raw["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new_raw["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new_raw["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        existing_norm = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str))
        df_new_raw["_is_new"] = ~df_new_raw["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_norm)
    else:
        df_new_raw = pd.DataFrame(columns=list(df_existing.columns))

    # ë³‘í•© + ì¤‘ë³µ ì œê±°
    combined = pd.concat([df_existing, df_new_raw], ignore_index=True)
    combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")
    combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
    combined["_ë°œí–‰ì¼_ì¼"] = combined["_ë°œí–‰ì¼_dt"].dt.date
    combined = combined.drop_duplicates(subset=["ì œëª©", "_ë°œí–‰ì¼_ì¼"], keep="first")

    # í‘œì‹œìš©
    out_cols = ["í‚¤ì›Œë“œ","ì œëª©","ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    df_all = combined[out_cols].copy()
    df_all.to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")

    # í‚¤ì›Œë“œë³„
    for kw, g in df_all.groupby("í‚¤ì›Œë“œ", sort=False):
        g.to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")

    # NEW
    today = datetime.now().strftime("%Y%m%d")
    df_new = combined.loc[combined["_is_new"]==True, out_cols].copy()
    df_new = df_new.sort_values(["ìˆ˜ì§‘ì‹œê°(KST)", "ë°œí–‰ì¼(KST)"], ascending=False)
    df_new.to_csv(DATA_DIR / f"NEW_{today}.csv", index=False, encoding="utf-8-sig")
    df_new.to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")

    print("ğŸ‰ CSV ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()

