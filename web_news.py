import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime
import re


# -----------------------------
# ì„¤ì •
# -----------------------------
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨"]     # í•„ìš” ì‹œ ìˆ˜ì •
DATA_DIR = Path("data")
LOG_DIR = Path("logs")
WRITE_LOG_FILE = True


# -----------------------------
# ìœ í‹¸
# -----------------------------
def make_session() -> requests.Session:
    sess = requests.Session()
    retries = Retry(
        total=5,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries)
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    sess.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NewsCrawler/1.0)"})
    return sess


def normalize_url(url: str) -> str:
    """êµ¬ê¸€ë‰´ìŠ¤ ë§í¬ ì¤‘ë³µ ì œê±°ìš© ì •ê·œí™”(utm_*, hl/gl/ceid/oc ì œê±°)"""
    try:
        parsed = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(parsed.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}:
                continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        new_query = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit(
            (parsed.scheme.lower(), parsed.netloc.lower(), parsed.path, new_query, "")
        )
    except Exception:
        return url


def extract_domain(url: str) -> str:
    try:
        return urllib.parse.urlsplit(url).netloc.lower()
    except Exception:
        return ""


def parse_pub_date(text: str):
    if not text:
        return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")


def to_kst_str_from_utc(ts):
    if pd.isna(ts):
        return None
    try:
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except Exception:
        return None


def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"


def resolve_final_url(session: requests.Session, url: str, timeout: float = 10.0) -> str:
    """
    êµ¬ê¸€ë‰´ìŠ¤ ë“± ë¦¬ë‹¤ì´ë ‰íŠ¸ ë§í¬ë¥¼ ì¶”ì í•˜ì—¬ ì‹¤ì œ ê¸°ì‚¬(ì›ë¬¸) URLì„ ë°˜í™˜.
    - HEADë¡œ ë¹ ë¥´ê²Œ ì‹œë„ í›„, í•„ìš”í•˜ë©´ GETë¡œ ì¬ì‹œë„
    - ì‹¤íŒ¨ ì‹œ ì›ë³¸ urlì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    try:
        r = session.head(url, allow_redirects=True, timeout=timeout)
        final_url = r.url
        if not final_url or final_url == url:
            r = session.get(url, allow_redirects=True, timeout=timeout)
            final_url = r.url
        return final_url or url
    except Exception:
        return url


# -----------------------------
# í¬ë¡¤ë§
# -----------------------------
def crawl_google_news_rss(session: requests.Session, keyword: str):
    encoded_kw = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=ko&gl=KR&ceid=KR:ko"

    resp = session.get(url, timeout=15)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "xml")  # lxml í•„ìš”
    items = soup.find_all("item")

    collected_at = pd.Timestamp.utcnow()
    rows = []
    for item in items:
        title = item.title.text if item.title else ""
        link = item.link.text if item.link else ""
        pub_date_raw = item.pubDate.text if item.pubDate else ""
        pub_ts = parse_pub_date(pub_date_raw)

        # ê¸´ êµ¬ê¸€ë‰´ìŠ¤ ë§í¬ â†’ ì›ë¬¸(ìµœì¢…) ë§í¬ë¡œ í•´ì„
        final_link = resolve_final_url(session, link)

        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            # ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì›ë³¸ ë§í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì œê±° í‚¤ë¡œ ì‚¬ìš©
            "_ì •ê·œí™”ë§í¬": normalize_url(link),
            # í‘œì‹œ/í•˜ì´í¼ë§í¬ìš©ì€ ì›ë¬¸ ë§í¬ ì‚¬ìš©
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": pub_ts,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at,
        })
    print(f"âœ… '{keyword}' ë‰´ìŠ¤ {len(rows)}ê±´ ìˆ˜ì§‘")
    return rows


# -----------------------------
# ë©”ì¸
# -----------------------------
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    log_lines = []
    t0 = time.time()
    ts_label = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 1) ê¸°ì¡´ ALL.csv ë¡œë“œ(ëˆ„ì )
    all_path = DATA_DIR / "ALL.csv"
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        for c in ["ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)"]:
            df_existing[c] = pd.to_datetime(df_existing[c], errors="coerce")
        # ë‚´ë¶€ìš© ì»¬ëŸ¼ ì¬ìƒì„±
        if "_ì •ê·œí™”ë§í¬" not in df_existing.columns:
            # ê³¼ê±° ë²„ì „ê³¼ í˜¸í™˜: ì›ë¬¸ë§í¬ê°€ ìˆìœ¼ë©´ ê·¸ê±¸, ì—†ìœ¼ë©´ ë¹ˆ ê°’
            df_existing["_ì •ê·œí™”ë§í¬"] = df_existing.get("ì›ë¬¸ë§í¬", pd.Series("", index=df_existing.index))
        df_existing["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_existing["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_existing["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_existing["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        df_existing["_is_new"] = False
    else:
        df_existing = pd.DataFrame(columns=[
            "í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜",
            "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"
        ])

    log_lines.append(f"[{ts_label}] ê¸°ì¡´ ALL.csv ë¡œë“œ: {len(df_existing)}ê±´")

    # 2) ì‹ ê·œ ìˆ˜ì§‘
    all_rows = []
    per_kw_counts = {}
    errors = []
    for kw in KEYWORDS:
        try:
            rows = crawl_google_news_rss(session, kw)
            per_kw_counts[kw] = len(rows)
            all_rows.extend(rows)
            time.sleep(0.5)
        except Exception as e:
            msg = f"âŒ '{kw}' ìˆ˜ì§‘ ì˜¤ë¥˜: {e}"
            print(msg)
            errors.append(msg)
    log_lines.append("ìˆ˜ì§‘ ê²°ê³¼(í‚¤ì›Œë“œë³„): " + ", ".join(f"{k}={v}" for k, v in per_kw_counts.items()))

    df_new_raw = pd.DataFrame(all_rows)
    if not df_new_raw.empty:
        df_new_raw["ë°œí–‰ì¼(KST)"] = df_new_raw["ë°œí–‰ì¼_UTC"].apply(to_kst_str_from_utc)
        df_new_raw["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new_raw["ìˆ˜ì§‘ì‹œê°_UTC"].apply(to_kst_str_from_utc)
        # ë‚´ë¶€ ì •ë ¬/ì¤‘ë³µ ì œê±°ìš© íƒ€ì„ìŠ¤íƒ¬í”„
        df_new_raw["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new_raw["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new_raw["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new_raw["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")

        existing_norm = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str))
        df_new_raw["_is_new"] = ~df_new_raw["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_norm)
    else:
        df_new_raw = pd.DataFrame(columns=list(df_existing.columns))

    # 3) ë³‘í•© + ì¤‘ë³µ ì œê±°(ìµœì‹  ìˆ˜ì§‘ ìš°ì„ )
    combined = pd.concat([df_existing, df_new_raw], ignore_index=True)
    combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")

    before = len(combined)
    combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
    combined["_ë°œí–‰ì¼_ì¼"] = combined["_ë°œí–‰ì¼_dt"].dt.date
    combined = combined.drop_duplicates(subset=["ì œëª©", "_ë°œí–‰ì¼_ì¼"], keep="first")
    after = len(combined)

    log_lines.append(f"ì¤‘ë³µ ì œê±°: {before} -> {after} (ì œê±° {before - after}ê±´)")

    # 4) í‘œì‹œìš© ì •ë ¬ ë° ì»¬ëŸ¼
    combined_display = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")
    out_cols = ["í‚¤ì›Œë“œ", "ì œëª©", "ì›ë¬¸ë§í¬", "ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)", "ì¶œì²˜"]
    df_all = combined_display[out_cols].copy()

    # 5) NEW (ì´ë²ˆ ì‹¤í–‰ ì‹ ê·œë§Œ)
    df_new_final = combined_display.loc[combined_display["_is_new"] == True, out_cols].copy()
    df_new_final = df_new_final.sort_values(["ìˆ˜ì§‘ì‹œê°(KST)", "ë°œí–‰ì¼(KST)"], ascending=False)

    # 6) CSV ì €ì¥
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    # ALL
    df_all.to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    # í‚¤ì›Œë“œë³„
    for kw, g in df_all.groupby("í‚¤ì›Œë“œ", sort=False):
        g.to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")
    # NEW
    today = datetime.now().strftime("%Y%m%d")
    df_new_final.to_csv(DATA_DIR / f"NEW_{today}.csv", index=False, encoding="utf-8-sig")
    df_new_final.to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")

    # 7) ë¡œê·¸(ì„ íƒ)
    elapsed = time.time() - t0
    log_lines.append(f"ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}s")
    if errors:
        log_lines.append("ì—ëŸ¬:"); log_lines.extend(errors)
    if WRITE_LOG_FILE:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        log_path = LOG_DIR / f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(log_path, "w", encoding="utf-8") as f:
            f.write("\n".join(log_lines))
        print(f"ğŸ§¾ ë¡œê·¸ ì €ì¥: {log_path}")

    print(f"ğŸ‰ CSV ì €ì¥ ì™„ë£Œ: {DATA_DIR.resolve()}")


if __name__ == "__main__":
    main()
