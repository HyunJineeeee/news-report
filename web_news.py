import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime
import re

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨"]
DATA_DIR = Path("data")

# ============== ìœ í‹¸ ==============
def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(
        total=5, backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"],
        raise_on_status=False,
    )
    ad = HTTPAdapter(max_retries=retries)
    s.mount("http://", ad); s.mount("https://", ad)
    s.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NewsCrawler/1.0)"})
    return s

def normalize_url(url: str) -> str:
    try:
        p = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(p.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}:
                continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        nq = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit((p.scheme.lower(), p.netloc.lower(), p.path, nq, ""))
    except Exception:
        return url

def extract_domain(url: str) -> str:
    try: return urllib.parse.urlsplit(url).netloc.lower()
    except: return ""

def parse_pub_date(text: str):
    if not text: return pd.NaT
    return pd.to_datetime(text, utc=True, errors="coerce")

def to_kst_str_from_utc(ts):
    if pd.isna(ts): return None
    try: return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except: return None

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

def resolve_final_url(session: requests.Session, url: str, timeout: float = 10.0) -> str:
    try:
        r = session.head(url, allow_redirects=True, timeout=timeout)
        fu = r.url
        if not fu or fu == url:
            r = session.get(url, allow_redirects=True, timeout=timeout)
            fu = r.url
        return fu or url
    except Exception:
        return url

# ============== í¬ë¡¤ë§ ==============
def crawl_google_news_rss(session: requests.Session, keyword: str):
    q = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    resp = session.get(url, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "xml")  # lxml í•„ìš”
    items = soup.find_all("item")

    collected_at = pd.Timestamp.utcnow()
    rows = []
    for it in items:
        title = it.title.text if it.title else ""
        link = it.link.text if it.link else ""
        pub_ts = parse_pub_date(it.pubDate.text if it.pubDate else "")

        final_link = resolve_final_url(session, link)
        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,                # í‘œì‹œ/í•˜ì´í¼ë§í¬ìš©
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": pub_ts,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at,
            "_ì •ê·œí™”ë§í¬": normalize_url(link),    # ì¤‘ë³µì œê±° í‚¤(êµ¬ê¸€ë‰´ìŠ¤ ë§í¬ ì •ê·œí™”)
        })
    print(f"âœ… '{keyword}' {len(rows)}ê±´")
    return rows

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    # 1) ê¸°ì¡´ ALL.csv ë¡œë“œ(ì—†ìœ¼ë©´ ë¹ˆ DF)
    all_path = DATA_DIR / "ALL.csv"
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        # ë¬¸ìì—´ì„ ë‚ ì§œë¡œ ë³µì›
        df_existing["ë°œí–‰ì¼(KST)"] = pd.to_datetime(df_existing["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_existing["ìˆ˜ì§‘ì‹œê°(KST)"] = pd.to_datetime(df_existing["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
    else:
        df_existing = pd.DataFrame(columns=["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"])

    # ë‚´ë¶€ìš© ì»¬ëŸ¼ ì¤€ë¹„
    if not df_existing.empty:
        df_existing["_ì •ê·œí™”ë§í¬"] = df_existing["ì›ë¬¸ë§í¬"].fillna("").apply(normalize_url)
        df_existing["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_existing["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_existing["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_existing["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        df_existing["_is_new"] = False

    # 2) ì‹ ê·œ ìˆ˜ì§‘
    all_rows = []
    for kw in KEYWORDS:
        all_rows.extend(crawl_google_news_rss(session, kw))
        time.sleep(0.5)
    df_new = pd.DataFrame(all_rows)

    if not df_new.empty:
        df_new["ë°œí–‰ì¼(KST)"] = df_new["ë°œí–‰ì¼_UTC"].apply(to_kst_str_from_utc)
        df_new["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new["ìˆ˜ì§‘ì‹œê°_UTC"].apply(to_kst_str_from_utc)
        df_new["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")

        # ì´ì „ ALL ê¸°ì¤€ 'ì‹ ê·œ' íŒì •
        existing_norm = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str)) if "_ì •ê·œí™”ë§í¬" in df_existing.columns else set()
        df_new["_is_new"] = ~df_new["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_norm)
    else:
        df_new = pd.DataFrame(columns=[
            "í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜",
            "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"
        ])

    # 3) ë³‘í•© + ì¤‘ë³µì œê±°(ìˆ˜ì§‘ ìµœì‹  ìš°ì„ )
    combined = pd.concat([df_existing, df_new], ignore_index=True)
    if not combined.empty:
        combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")
        combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")
        combined["_ë°œí–‰ì¼_ì¼"] = combined["_ë°œí–‰ì¼_dt"].dt.date
        combined = combined.drop_duplicates(subset=["ì œëª©","_ë°œí–‰ì¼_ì¼"], keep="first")

    # 4) í‘œì‹œìš© DF + ë‚ ì§œ ë°±í•„(ë¹ˆ ê°’ ë°©ì§€)
    out_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    combined_display = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")
    df_all = combined_display[out_cols].copy()

    # ë¹ˆ ë‚ ì§œ ë³´ì •: ë°œí–‰ì¼ ì—†ìœ¼ë©´ ìˆ˜ì§‘ì‹œê°ìœ¼ë¡œ, ìˆ˜ì§‘ì‹œê° ì—†ìœ¼ë©´ ì§€ê¸ˆ(KST)
    def fmt(s): return pd.to_datetime(s, errors="coerce").dt.strftime("%Y-%m-%d %H:%M")
    df_all["ë°œí–‰ì¼(KST)"] = df_all["ë°œí–‰ì¼(KST)"].where(df_all["ë°œí–‰ì¼(KST)"].notna(), fmt(combined_display["_ë°œí–‰ì¼_dt"]))
    df_all["ë°œí–‰ì¼(KST)"] = df_all["ë°œí–‰ì¼(KST)"].where(df_all["ë°œí–‰ì¼(KST)"].notna(), fmt(combined_display["_ìˆ˜ì§‘ì‹œê°_dt"]))
    df_all["ìˆ˜ì§‘ì‹œê°(KST)"] = df_all["ìˆ˜ì§‘ì‹œê°(KST)"].where(df_all["ìˆ˜ì§‘ì‹œê°(KST)"].notna(), fmt(combined_display["_ìˆ˜ì§‘ì‹œê°_dt"]))
    df_all["ìˆ˜ì§‘ì‹œê°(KST)"] = df_all["ìˆ˜ì§‘ì‹œê°(KST)"].fillna(pd.Timestamp.now(tz="Asia/Seoul").strftime("%Y-%m-%d %H:%M"))

    # NEW: ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì‹ ê·œë§Œ
    df_new_final = combined_display.loc[combined_display["_is_new"] == True, out_cols].copy()
    df_new_final = df_new_final.sort_values(["ìˆ˜ì§‘ì‹œê°(KST)","ë°œí–‰ì¼(KST)"], ascending=False)

    # 5) ì €ì¥
    df_all.to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    for kw, g in df_all.groupby("í‚¤ì›Œë“œ", sort=False):
        g.to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")
    df_new_final.to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")


    print("ğŸ‰ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
