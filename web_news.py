# web_news.py
import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib.parse
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨", "ê³ ìš©ë…¸ë™ë¶€", "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨"]
DATA_DIR = Path("data")

# ============== ìœ í‹¸ ==============
def make_session() -> requests.Session:
    s = requests.Session()
    retries = Retry(
        total=5,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "HEAD"],
        raise_on_status=False,
    )
    ad = HTTPAdapter(max_retries=retries)
    s.mount("http://", ad)
    s.mount("https://", ad)
    s.headers.update({"User-Agent": "Mozilla/5.0 (compatible; NewsCrawler/1.0)"})
    return s

def normalize_url(url: str) -> str:
    """êµ¬ê¸€ë‰´ìŠ¤ ë§í¬ ì •ê·œí™”(utm_*, hl/gl/ceid/oc ì œê±°)"""
    try:
        p = urllib.parse.urlsplit(url)
        q = urllib.parse.parse_qsl(p.query, keep_blank_values=True)
        kept = []
        for k, v in q:
            kl = k.lower()
            if kl.startswith("utm_") or kl in {"hl", "gl", "ceid", "oc"}:
                continue
            kept.append((k, v))
        kept.sort(key=lambda x: x[0])
        nq = urllib.parse.urlencode(kept, doseq=True)
        return urllib.parse.urlunsplit((p.scheme.lower(), p.netloc.lower(), p.path, nq, ""))
    except Exception:
        return url

def extract_domain(url: str) -> str:
    try:
        return urllib.parse.urlsplit(url).netloc.lower()
    except Exception:
        return ""

def parse_pub_date(text: str):
    """RSS pubDate â†’ UTC pandas.Timestamp (ì‹¤íŒ¨ ì‹œ NaT)"""
    if not text:
        return pd.NaT
    # RFC822 ë¬¸ìì—´ì„ ê´€ëŒ€í•˜ê²Œ íŒŒì‹± + UTCë¡œ ê³ ì •
    return pd.to_datetime(text, utc=True, errors="coerce")

def utc_to_kst_str(utc_ts):
    """UTC Timestamp/str â†’ KST ë¬¸ìì—´ 'YYYY-MM-DD HH:MM' (ê²°ì¸¡ì€ '')"""
    if utc_ts is None or pd.isna(utc_ts):
        return ""
    try:
        ts = pd.to_datetime(utc_ts, utc=True, errors="coerce")
        if pd.isna(ts):
            return ""
        return ts.tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    except Exception:
        return ""

def safe_name(name: str) -> str:
    return re.sub(r"[\\/:*?\[\]]", "_", str(name))[:64] or "Sheet"

def resolve_final_url(session: requests.Session, url: str, timeout: float = 8.0) -> str:
    """ë¦¬ë‹¤ì´ë ‰íŠ¸ë¥¼ ë”°ë¼ê°€ ìµœì¢…(ì›ë¬¸) URL ë°˜í™˜. ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€."""
    try:
        r = session.head(url, allow_redirects=True, timeout=timeout)
        fu = r.url
        if not fu or fu == url:
            r = session.get(url, allow_redirects=True, timeout=timeout)
            fu = r.url
        return fu or url
    except Exception:
        return url

# ============== í¬ë¡¤ë§ ==============
def crawl_google_news_rss(session: requests.Session, keyword: str):
    q = urllib.parse.quote(keyword)
    url = f"https://news.google.com/rss/search?q={q}&hl=ko&gl=KR&ceid=KR:ko"
    resp = session.get(url, timeout=20)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "xml")  # lxml í•„ìš”
    items = soup.find_all("item")

    collected_at_utc = pd.Timestamp.utcnow()
    rows = []
    for it in items:
        title = it.title.text if it.title else ""
        link = it.link.text if it.link else ""
        pub_ts_utc = parse_pub_date(it.pubDate.text if it.pubDate else "")

        # ìµœì¢… URL í•´ì„(ë¦¬ë‹¤ì´ë ‰íŠ¸) â€” ì‹¤íŒ¨í•´ë„ ì›ë³¸ ìœ ì§€
        final_link = resolve_final_url(session, link)

        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": extract_domain(final_link) or extract_domain(link),
            "ë°œí–‰ì¼_UTC": pub_ts_utc,
            "ìˆ˜ì§‘ì‹œê°_UTC": collected_at_utc,
            "_ì •ê·œí™”ë§í¬": normalize_url(link),  # êµ¬ê¸€ë‰´ìŠ¤ ê¸°ì¤€ ì¤‘ë³µ ì œê±° í‚¤
        })
    print(f"âœ… '{keyword}' {len(rows)}ê±´")
    return rows

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    session = make_session()

    # 1) ê¸°ì¡´ ALL.csv ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ ìŠ¤í‚¤ë§ˆ ìƒì„±)
    all_path = DATA_DIR / "ALL.csv"
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        # ëˆ„ë½ ì»¬ëŸ¼ ë³´ì •
        for col in ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]:
            if col not in df_existing.columns:
                df_existing[col] = ""
        # ë‚´ë¶€ dt ì»¬ëŸ¼ ì¤€ë¹„
        df_existing["_ì •ê·œí™”ë§í¬"] = df_existing.get("ì›ë¬¸ë§í¬", "").astype(str).apply(normalize_url)
        df_existing["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_existing["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_existing["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_existing["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        df_existing["_is_new"] = False
    else:
        df_existing = pd.DataFrame(columns=[
            "í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜",
            "_ì •ê·œí™”ë§í¬","_ë°œí–‰ì¼_dt","_ìˆ˜ì§‘ì‹œê°_dt","_is_new"
        ])

    # 2) ì‹ ê·œ ìˆ˜ì§‘
    all_rows = []
    for kw in KEYWORDS:
        try:
            all_rows.extend(crawl_google_news_rss(session, kw))
        except Exception as e:
            print(f"[WARN] '{kw}' ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        time.sleep(0.4)
    df_new = pd.DataFrame(all_rows)

    if not df_new.empty:
        # UTC â†’ KST ë¬¸ìì—´ ë³€í™˜ (ë¹ˆê°’ì€ "")
        df_new["ë°œí–‰ì¼(KST)"] = df_new["ë°œí–‰ì¼_UTC"].apply(utc_to_kst_str)
        df_new["ìˆ˜ì§‘ì‹œê°(KST)"] = df_new["ìˆ˜ì§‘ì‹œê°_UTC"].apply(utc_to_kst_str)
        # ë‚´ë¶€ dt
        df_new["_ë°œí–‰ì¼_dt"] = pd.to_datetime(df_new["ë°œí–‰ì¼(KST)"], errors="coerce")
        df_new["_ìˆ˜ì§‘ì‹œê°_dt"] = pd.to_datetime(df_new["ìˆ˜ì§‘ì‹œê°(KST)"], errors="coerce")
        # ì‹ ê·œ íŒë‹¨(ì •ê·œí™” ë§í¬ ê¸°ì¤€)
        existing_norm = set(df_existing["_ì •ê·œí™”ë§í¬"].dropna().astype(str)) if "_ì •ê·œí™”ë§í¬" in df_existing.columns else set()
        df_new["_is_new"] = ~df_new["_ì •ê·œí™”ë§í¬"].astype(str).isin(existing_norm)
    else:
        df_new = pd.DataFrame(columns=df_existing.columns)

    # 3) ë³‘í•© + ì¤‘ë³µì œê±°(ìˆ˜ì§‘ ìµœì‹  ìš°ì„ )
    combined = pd.concat([df_existing, df_new], ignore_index=True)
    if not combined.empty:
        combined = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")
        combined = combined.drop_duplicates(subset=["_ì •ê·œí™”ë§í¬"], keep="first")  # ë§í¬ ê¸°ë°˜
        # ê°™ì€ ì œëª© & ê°™ì€ 'ë°œí–‰ì¼(ì¼ ë‹¨ìœ„)'ëŠ” ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        combined["_ë°œí–‰ì¼_ì¼"] = combined["_ë°œí–‰ì¼_dt"].dt.date
        combined = combined.drop_duplicates(subset=["ì œëª©", "_ë°œí–‰ì¼_ì¼"], keep="first")

    # 4) í‘œì‹œ/ì €ì¥ìš© DF (6ê°œ ì»¬ëŸ¼ë§Œ, ìˆœì„œ ê³ ì •, ê²°ì¸¡ì€ ë¹ˆì¹¸)
    out_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
    combined_display = combined.sort_values("_ìˆ˜ì§‘ì‹œê°_dt", ascending=False, na_position="last")

    df_all = combined_display[out_cols].copy()
    # ìˆ˜ì§‘ì‹œê°ì€ ë‚´ë¶€ dtë¡œ ë³´ì •(ë¹„ì—ˆìœ¼ë©´ ì±„ì›€) â†’ ë¬¸ìì—´(24h) ê°•ì œ
    backfill_collect = pd.to_datetime(combined_display["_ìˆ˜ì§‘ì‹œê°_dt"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M")
    df_all["ìˆ˜ì§‘ì‹œê°(KST)"] = df_all["ìˆ˜ì§‘ì‹œê°(KST)"].where(df_all["ìˆ˜ì§‘ì‹œê°(KST)"].notna(), backfill_collect)
    df_all["ìˆ˜ì§‘ì‹œê°(KST)"] = df_all["ìˆ˜ì§‘ì‹œê°(KST)"].fillna(pd.Timestamp.utcnow().tz_localize("UTC").tz_convert("Asia/Seoul").strftime("%Y-%m-%d %H:%M"))

    # ë°œí–‰ì¼/ìˆ˜ì§‘ì‹œê°ì€ í•­ìƒ ë¬¸ìì—´, NaT/NaN â†’ ""
    for col in ["ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)"]:
        ser = pd.to_datetime(df_all[col], errors="coerce")
        df_all[col] = ser.dt.strftime("%Y-%m-%d %H:%M")
        df_all[col] = df_all[col].fillna("")

    # NEW: ì´ë²ˆ ì‹¤í–‰ì—ì„œ 'ì‹ ê·œ'ë§Œ
    df_new_final = combined_display.loc[combined_display["_is_new"] == True, out_cols].copy()
    for col in ["ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)"]:
        ser = pd.to_datetime(df_new_final[col], errors="coerce")
        df_new_final[col] = ser.dt.strftime("%Y-%m-%d %H:%M")
        df_new_final[col] = df_new_final[col].fillna("")

    # 5) ì €ì¥ (UTF-8 with BOM, ì—‘ì…€ í˜¸í™˜)
    df_all.to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    for kw, g in df_all.groupby("í‚¤ì›Œë“œ", sort=False):
        g.to_csv(DATA_DIR / f"{safe_name(kw)}.csv", index=False, encoding="utf-8-sig")
    df_new_final.to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")

    print("ğŸ‰ ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    main()
