# web_news.py
import requests
import pandas as pd
import os
import smtplib
import time
import re
from pathlib import Path
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai
from datetime import datetime, timedelta
import trafilatura  # â˜… ë” ê°•ë ¥í•œ ë³¸ë¬¸ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ============== ì„¤ì • ==============
KEYWORDS = ["ì¼í•™ìŠµë³‘í–‰", "ì§ì—…í›ˆë ¨", "ê³ ìš©ë…¸ë™ë¶€", "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨"]
DATA_DIR = Path("data")

# â˜… ìš”ì²­í•˜ì‹  ìƒ‰ìƒ ì ìš©
KEYWORD_COLORS = {
    "ì¼í•™ìŠµë³‘í–‰": "#3498db",      # íŒŒë‘
    "ì§ì—…í›ˆë ¨": "#e67e22",        # ì£¼í™©
    "ê³ ìš©ë…¸ë™ë¶€": "#7f8c8d",      # íšŒìƒ‰
    "í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨": "#2c3e50" # ë‚¨ìƒ‰ (ê³µë‹¨)
}

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
EMAIL_USER = os.environ.get("EMAIL_USER")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ============== ìœ í‹¸ ==============
def clean_html(raw_html):
    """HTML íƒœê·¸ ì œê±°"""
    if not raw_html: return ""
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext.replace("&quot;", "'").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")

def normalize_title(title):
    """ì¤‘ë³µ ì œê±°ìš©: íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±ì„ ëª¨ë‘ ì œê±°í•œ ìˆœìˆ˜ í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¹€"""
    # ì˜ˆ: "[ë‹¨ë…] ë‰´ìŠ¤!" -> "ë‹¨ë…ë‰´ìŠ¤"
    return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title)

# ============== AI & ë³¸ë¬¸ ì¶”ì¶œ (ì—…ê·¸ë ˆì´ë“œ) ==============
def extract_article_content(url: str) -> str:
    """
    Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Newspaper3kë³´ë‹¤ ìµœì‹  ì‚¬ì´íŠ¸ ëŒ€ì‘ë ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.
    """
    if not url: return ""
    try:
        # 1. Trafilaturaë¡œ ë‹¤ìš´ë¡œë“œ ë° ì¶”ì¶œ
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            if text and len(text) >= 100:
                return text
        
        # 2. ì‹¤íŒ¨ ì‹œ ì¼ë°˜ Requestsë¡œ ì¬ì‹œë„ (í—¤ë” ì¶”ê°€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code == 200:
            # ë‹¤ì‹œ Trafilaturaë¡œ íŒŒì‹± ì‹œë„
            text = trafilatura.extract(resp.text, include_comments=False)
            if text and len(text) >= 100:
                return text
                
        return ""
    except Exception:
        return ""

def summarize_with_gemini(text: str) -> str:
    if not GEMINI_API_KEY or not text: return ""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = (
            "ë„ˆëŠ” ê³µê³µê¸°ê´€ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ì‘ì„± ë´‡ì´ì•¼. ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì½ê³  í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´.\n"
            "ì¡°ê±´ 1: '- 'ë¡œ ì‹œì‘í•˜ëŠ” ê°œì¡°ì‹(bullet point) ë¬¸ì¥.\n"
            "ì¡°ê±´ 2: í˜•ìš©ì‚¬ë¥¼ ë°°ì œí•˜ê³  'ì‚¬ì‹¤(Fact)'ê³¼ 'ìˆ˜ì¹˜' ìœ„ì£¼ë¡œ ê±´ì¡°í•˜ê²Œ ì‘ì„±.\n"
            "ì¡°ê±´ 3: ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê´‘ê³ ì„±ì´ë¼ë©´ 'ìš”ì•½í•  ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤'ë¼ê³  ì¶œë ¥.\n\n"
            f"ê¸°ì‚¬ ë³¸ë¬¸:\n{text[:5000]}"
        )
        response = model.generate_content(prompt)
        return response.text.strip()
    except: return ""

# ============== ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ==============
def crawl_naver_news(keyword, target_date_str):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("[ERROR] ë„¤ì´ë²„ API í‚¤ ëˆ„ë½")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    params = {
        "query": keyword,
        "display": 100, 
        "start": 1,
        "sort": "date"
    }

    try:
        resp = requests.get(url, headers=headers, params=params)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        print(f"   [API Error] {e}")
        return []

    rows = []
    collected_at = pd.Timestamp.now(tz="Asia/Seoul").strftime("%Y-%m-%d %H:%M")
    
    for item in data.get('items', []):
        try:
            pub_date_dt = datetime.strptime(item['pubDate'], "%a, %d %b %Y %H:%M:%S +0900")
            pub_date_str = pub_date_dt.strftime("%Y-%m-%d %H:%M")
            pub_date_day = pub_date_dt.strftime("%Y-%m-%d")
        except: continue

        if pub_date_day != target_date_str:
            continue
            
        final_link = item['originallink'] if item['originallink'] else item['link']
        if not final_link: continue

        title = clean_html(item['title'])
        desc = clean_html(item['description'])
        
        rows.append({
            "í‚¤ì›Œë“œ": keyword,
            "ì œëª©": title,
            "ì›ë¬¸ë§í¬": final_link,
            "ì¶œì²˜": "NaverAPI",
            "ë°œí–‰ì¼(KST)": pub_date_str,
            "ìˆ˜ì§‘ì‹œê°(KST)": collected_at,
            "ìš”ì•½": "",
            "_api_desc": desc,
            "_title_norm": normalize_title(title) # â˜… ì¤‘ë³µì œê±°ì˜ í•µì‹¬ í‚¤
        })
        
    return rows

# ============== ì´ë©”ì¼ ë°œì†¡ ==============
def send_email_report(df_new, target_date_str):
    if not EMAIL_USER or not EMAIL_PASSWORD or not EMAIL_RECEIVER: return
    if df_new.empty: return

    subject = f"[ì¼ë³‘ë¦¬í¬íŠ¸] {target_date_str} ì£¼ìš” ë‰´ìŠ¤ ì•Œë¦¼"

    html_body = f"""
    <div style="font-family: 'Malgun Gothic', sans-serif; background-color: #f4f4f4; padding: 20px; color: #333;">
        <div style="max-width: 700px; margin: 0 auto; background-color: #ffffff; padding: 30px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.05);">
            <div style="text-align: center; margin-bottom: 30px; border-bottom: 2px solid #333; padding-bottom: 20px;">
                <h1 style="color: #2c3e50; font-size: 24px; margin: 0;">ğŸ“° {target_date_str} ë‰´ìŠ¤ ë¦¬í¬íŠ¸</h1>
                <p style="color: #7f8c8d; font-size: 14px; margin-top: 10px;">
                    ì–´ì œ ìˆ˜ì§‘ëœ ì´ <span style="color:#e67e22; font-weight:bold;">{len(df_new)}</span>ê±´ì˜ ê¸°ì‚¬ ìš”ì•½ì…ë‹ˆë‹¤.
                </p>
            </div>
    """

    grouped = df_new.groupby("í‚¤ì›Œë“œ")
    for kw in KEYWORDS:
        if kw in grouped.groups:
            group_df = grouped.get_group(kw)
            
            kw_color = KEYWORD_COLORS.get(kw, "#333333")
            
            html_body += f"""
            <div style="margin-bottom: 30px;">
                <div style="background-color: {kw_color}; color: white; padding: 6px 15px; display: inline-block; border-radius: 15px; font-weight: bold; font-size: 16px; margin-bottom: 15px;">
                    # {kw}
                </div>
            """
            for idx, row in group_df.iterrows():
                title = row['ì œëª©']
                link = row['ì›ë¬¸ë§í¬']
                date = row['ë°œí–‰ì¼(KST)']
                summary = row['ìš”ì•½']

                # ìš”ì•½ HTML
                summary_html = summary.replace('\n', '<br>')
                # ìš”ì•½ì´ ìˆìœ¼ë©´ í•´ë‹¹ í‚¤ì›Œë“œ ìƒ‰ìƒ í…Œë‘ë¦¬, ì—†ìœ¼ë©´ íšŒìƒ‰
                border_color = kw_color if "- " in summary else "#ddd"
                
                html_body += f"""
                <div style="border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 15px; background-color: #fff;">
                    <a href="{link}" target="_blank" style="font-size: 18px; font-weight: bold; color: #2c3e50; text-decoration: none; display: block; margin-bottom: 8px; line-height: 1.4;">
                        {title}
                    </a>
                    <div style="font-size: 12px; color: #95a5a6; margin-bottom: 15px;">
                        {date}
                    </div>
                    <div style="background-color: #f9f9f9; padding: 15px; border-left: 4px solid {border_color}; color: #555; font-size: 14px; line-height: 1.6; border-radius: 4px;">
                        {summary_html}
                    </div>
                    <div style="text-align: right; margin-top: 10px;">
                        <a href="{link}" target="_blank" style="display: inline-block; background-color: #ecf0f1; color: #555; padding: 5px 12px; border-radius: 4px; text-decoration: none; font-size: 12px;">
                            ì›ë¬¸ ë³´ëŸ¬ê°€ê¸° â†’
                        </a>
                    </div>
                </div>
                """
            html_body += '</div>'

    html_body += """
            <div style="text-align: center; margin-top: 40px; font-size: 12px; color: #bdc3c7; border-top: 1px solid #eee; padding-top: 20px;">
                Automated by GitHub Actions & Naver API
            </div>
        </div>
    </div>
    """

    try:
        msg = MIMEMultipart()
        msg['Subject'] = subject
        msg['From'] = EMAIL_USER
        msg['To'] = EMAIL_RECEIVER
        msg.attach(MIMEText(html_body, 'html'))

        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(EMAIL_USER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! ({subject})")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

# ============== ë©”ì¸ ==============
def main():
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    now_kst = pd.Timestamp.now(tz="Asia/Seoul")
    yesterday_kst = now_kst - pd.Timedelta(days=1)
    target_date_str = yesterday_kst.strftime("%Y-%m-%d")
    print(f"ğŸ¯ íƒ€ê²Ÿ ë‚ ì§œ(ì–´ì œ): {target_date_str}")

    all_path = DATA_DIR / "ALL.csv"
    req_cols = ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜","ìš”ì•½","_api_desc","_title_norm"]
    
    if all_path.exists():
        df_existing = pd.read_csv(all_path, dtype=str, encoding="utf-8-sig")
        for c in req_cols: 
            if c not in df_existing.columns: df_existing[c] = ""
        existing_titles = set(df_existing["_title_norm"].dropna().astype(str))
    else:
        df_existing = pd.DataFrame(columns=req_cols)
        existing_titles = set()

    raw_rows = []
    for kw in KEYWORDS:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘ (Naver): {kw}...")
        raw_rows.extend(crawl_naver_news(kw, target_date_str))
        time.sleep(0.5)
    
    if not raw_rows: 
        print(f"ğŸ“… {target_date_str} ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df_crawled = pd.DataFrame(raw_rows)
    
    # â˜… ì¤‘ë³µ ì œê±°: ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ì œëª©ê³¼ ê²¹ì¹˜ë©´ ì œì™¸
    df_crawled["_is_new"] = ~df_crawled["_title_norm"].astype(str).isin(existing_titles)
    # ì´ë²ˆ ìˆ˜ì§‘ ë‚´ì—ì„œë„ ì œëª© ì¤‘ë³µ ì œê±°
    df_crawled = df_crawled.drop_duplicates(subset=["_title_norm"], keep="first")
    
    df_to_process = df_crawled[df_crawled["_is_new"] == True].copy()
    print(f"ğŸ” {target_date_str} ê¸°ì‚¬ ì¤‘ ì‹ ê·œ {len(df_to_process)}ê±´ ë°œê²¬.")

    processed_rows = []
    for idx, row in df_to_process.iterrows():
        print(f"   Processing: {row['ì œëª©'][:20]}...")
        real_url = row["ì›ë¬¸ë§í¬"]
        keyword = row["í‚¤ì›Œë“œ"]
        api_desc = row["_api_desc"]
        
        # 1. Trafilaturaë¡œ ë³¸ë¬¸ ì¶”ì¶œ
        content = extract_article_content(real_url)
        summary = ""
        
        # 2. ë³¸ë¬¸ì´ ìˆìœ¼ë©´ AI ìš”ì•½ ì‹œë„
        if content:
            # ë³¸ë¬¸ì— í‚¤ì›Œë“œ í™•ì¸ (ì •í™•ë„ í–¥ìƒ)
            if keyword not in content and keyword not in row['ì œëª©']:
                print(f"   âŒ [ì œì™¸] ë³¸ë¬¸ì— '{keyword}' ì—†ìŒ")
                continue 

            summary = summarize_with_gemini(content)
            time.sleep(2) # API ì†ë„ ì¡°ì ˆ
        
        # 3. ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ -> ë„¤ì´ë²„ API ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´ (ì ˆëŒ€ ë¹ˆì¹¸ X)
        if not summary or "ë¶€ì¡±í•©ë‹ˆë‹¤" in summary:
            if api_desc:
                # API ì„¤ëª…ì´ë¼ë„ ê¹”ë”í•˜ê²Œ í‘œì‹œ
                summary = f"- (ë³¸ë¬¸ ì ‘ì† ë¶ˆê°€ë¡œ ìš”ì•½ ëŒ€ì²´) {api_desc}..."
            else:
                summary = "- ìš”ì•½í•  ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            
        row["ìš”ì•½"] = summary
        processed_rows.append(row)

    if processed_rows:
        df_new_processed = pd.DataFrame(processed_rows)
        send_email_report(df_new_processed, target_date_str)
    else:
        print("ğŸ§¹ ì²˜ë¦¬í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        df_new_processed = pd.DataFrame(columns=df_crawled.columns)

    df_final_new = df_new_processed[req_cols] if not df_new_processed.empty else pd.DataFrame(columns=req_cols)
    combined = pd.concat([df_existing, df_final_new], ignore_index=True)
    combined = combined.drop_duplicates(subset=["_title_norm"], keep="last")
    combined = combined.sort_values("ìˆ˜ì§‘ì‹œê°(KST)", ascending=False)

    display_cols = ["í‚¤ì›Œë“œ","ì œëª©","ìš”ì•½","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)"]
    combined[display_cols].to_csv(DATA_DIR / "ALL.csv", index=False, encoding="utf-8-sig")
    
    if not df_new_processed.empty:
        df_new_processed[display_cols].to_csv(DATA_DIR / "NEW_latest.csv", index=False, encoding="utf-8-sig")
    
    print("ğŸ‰ ì™„ë£Œ")

if __name__ == "__main__":
    main()
