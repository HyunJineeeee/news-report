# -*- coding: utf-8 -*-
"""
news_pipeline.py (ì •ë¦¬ ìŠ¬ë¦¼ ë²„ì „ + ë²„ê·¸í”½ìŠ¤)
---------------------------------------
- append-only ì›ë³¸ ëˆ„ì (raw)
- ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° â†’ ëŒ€í‘œ ê¸°ì‚¬ 1ê±´ ì„ ì •(master)
- ì €ì¥ ì‹œ ì»¬ëŸ¼ ìˆœì„œ ê³ ì •:
  ["í‚¤ì›Œë“œ","ì œëª©","ì›ë¬¸ë§í¬","ë°œí–‰ì¼(KST)","ìˆ˜ì§‘ì‹œê°(KST)","ì¶œì²˜"]
- í¬ì¸íŠ¸:
  * collected_atì„ tz-aware(KST)ë¡œ ê¸°ë¡
  * ë¬¸ìì—´ ì»¬ëŸ¼ì€ fillna("") í›„ astype(str) â†’ "nan" ë°©ì§€
  * ì…ë ¥ CSV í—¤ë” í‘œì¤€í™” + ì¤‘ë³µ í—¤ë” ë³‘í•©
  * data/*.csv ë¡œë”© ì‹œ news_raw.csv/news_master.csv ì œì™¸
"""

import argparse
import glob
import os
from urllib.parse import urlparse

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors


# ---------------------------------------------------------------------
# ì„¤ì • / ì»¬ëŸ¼ ì •ì˜
# ---------------------------------------------------------------------
KST_TZ = "Asia/Seoul"
OUTPUT_ORDER = ["í‚¤ì›Œë“œ", "ì œëª©", "ì›ë¬¸ë§í¬", "ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)", "ì¶œì²˜"]

COL_CANDIDATES = {
    "keyword": ["keyword", "í‚¤ì›Œë“œ"],
    "title": ["title", "subject", "headline", "ì œëª©"],
    "url": ["url", "link", "ê¸°ì‚¬ë§í¬", "news_url", "ì›ë¬¸ë§í¬"],
    "pub_date": [
        "pub_date", "date", "published_at", "publish_date",
        "ë°œí–‰ì¼", "ì‘ì„±ì¼", "ë°œí–‰ì¼(KST)", "ë“±ë¡ì¼", "ê²Œì‹œì¼"
    ],
    "press": ["press", "source", "publisher", "ì–¸ë¡ ì‚¬", "ë§¤ì²´", "ì¶œì²˜"],
    "collected_at": [
        "collected_at", "staged_at", "crawled_at",
        "ìˆ˜ì§‘ì¼", "ìˆ˜ì§‘ì¼ì", "ìˆ˜ì§‘ì‹œê°", "ìˆ˜ì§‘ì‹œê°(KST)"
    ]
}



# ---------------------------------------------------------------------
# ìœ í‹¸
# ---------------------------------------------------------------------
def extract_domain(u: str) -> str:
    try:
        return urlparse(str(u)).netloc.lower()
    except Exception:
        return ""


def to_kst_ts(ts):
    """tz-aware/naive ëª¨ë“  ì…ë ¥ì„ KST tz-awareë¡œ ë³€í™˜"""
    if pd.isna(ts):
        return pd.NaT
    ts = pd.to_datetime(ts, errors="coerce")
    if pd.isna(ts):
        return pd.NaT
    try:
        if ts.tzinfo is None or ts.tzinfo.utcoffset(ts) is None:
            return ts.tz_localize(KST_TZ)
        else:
            return ts.tz_convert(KST_TZ)
    except Exception:
        return pd.NaT


def format_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì €ì¥ìš© 6ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë¦¬ (ìˆœì„œ ê³ ì •, ê³µë°± ì²˜ë¦¬, KST í¬ë§·)"""
    out = pd.DataFrame(index=df.index.copy())
    out["í‚¤ì›Œë“œ"] = df.get("keyword", "").fillna("").astype(str)
    out["ì œëª©"] = df.get("title", "").fillna("").astype(str)
    out["ì›ë¬¸ë§í¬"] = df.get("url", "").fillna("").astype(str)
    out["ì¶œì²˜"] = df.get("press", "").fillna("").astype(str)

    pub_kst = pd.to_datetime(df.get("pub_date", pd.NaT), errors="coerce").apply(to_kst_ts)
    col_kst = pd.to_datetime(df.get("collected_at", pd.NaT), errors="coerce").apply(to_kst_ts)

    out["ë°œí–‰ì¼(KST)"] = pub_kst.dt.strftime("%Y-%m-%d %H:%M").fillna("")
    out["ìˆ˜ì§‘ì‹œê°(KST)"] = col_kst.dt.strftime("%Y-%m-%d %H:%M").fillna("")

    return out.loc[:, OUTPUT_ORDER]


def map_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì…ë ¥ CSVì˜ ë‹¤ì–‘í•œ í—¤ë”ë¥¼ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ ì •ê·œí™” + ì¤‘ë³µ í—¤ë” ë³‘í•© + í—¤ë” í´ë¦°ì—…"""
    # 0) ğŸ”§ í—¤ë” í´ë¦°ì—…: ì•ë’¤ ê³µë°± ì œê±° + BOM ì œê±°
    if len(df.columns):
        cleaned = {}
        for c in df.columns:
            nc = str(c)
            nc = nc.replace("\ufeff", "")  # BOM ì œê±°
            nc = nc.strip()                # ì•ë’¤ ê³µë°± ì œê±°
            cleaned[c] = nc
        if any(k != v for k, v in cleaned.items()):
            df = df.rename(columns=cleaned)

    cols_lower = {c.lower(): c for c in df.columns}

    def choose(cands):
        for cand in cands:
            # ì •í™• ì¼ì¹˜
            if cand in df.columns:
                return cand
            # ì†Œë¬¸ì ë¹„êµ(ê³µë°±/BOM ì œê±° í›„)
            cl = cand.lower()
            if cl in cols_lower:
                return cols_lower[cl]
        return None

    # 1) í‘œì¤€ëª…ìœ¼ë¡œ ë¦¬ë„¤ì„(ì—†ìœ¼ë©´ ìƒì„±)
    for std, cands in COL_CANDIDATES.items():
        chosen = choose(cands)
        if chosen is None:
            df[std] = pd.Series([pd.NA] * len(df))
        else:
            if chosen != std:
                df.rename(columns={chosen: std}, inplace=True)

    # 2) ì¤‘ë³µ í—¤ë” ë³‘í•© (ë¨¼ì € ì±„ì›Œì§„ ê°’ ìš°ì„ )
    if df.columns.duplicated().any():
        dup_names = set(df.columns[df.columns.duplicated(keep=False)])
        for name in dup_names:
            same_cols = [i for i, c in enumerate(df.columns) if c == name]
            merged = df.iloc[:, same_cols].bfill(axis=1).iloc[:, 0]
            df.drop(columns=[df.columns[i] for i in same_cols], inplace=True)
            df[name] = merged

    # 3) ë‚ ì§œ íŒŒì‹± (ë¬¸ìì—´ â†’ Timestamp)
    for dcol in ["pub_date", "collected_at"]:
        if dcol in df.columns:
            ser = df[dcol]
            if isinstance(ser, pd.DataFrame):
                ser = ser.bfill(axis=1).iloc[:, 0]
            df[dcol] = pd.to_datetime(ser, errors="coerce")

    # 4) press ì—†ìœ¼ë©´ URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
    if "press" in df.columns and "url" in df.columns:
        df["press"] = df["press"].fillna("")
        empty_press = df["press"].astype(str).str.strip().eq("")
        if empty_press.any():
            df.loc[empty_press, "press"] = df.loc[empty_press, "url"].apply(extract_domain)

    return df



# ---------------------------------------------------------------------
# ì ì¬ / append-only
# ---------------------------------------------------------------------
def load_csvs(patterns):
    """ì…ë ¥ CSV ë¡œë”© (ì¶œë ¥ íŒŒì¼ ì œì™¸)"""
    paths, frames = [], []
    ignore = {"news_raw.csv", "news_master.csv"}
    for p in patterns:
        paths.extend(glob.glob(p))
    for path in sorted(set(paths)):
        base = os.path.basename(path)
        if base in ignore:
            continue
        try:
            df = pd.read_csv(path)
            frames.append(df)
        except Exception as e:
            print(f"[WARN] CSV ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()


def anti_join_by_url_title(raw: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    """ë™ì¼ URL+ì œëª© 1ì°¨ ì¤‘ë³µ ì œê±° (ì„±ëŠ¥/ìš©ëŸ‰ ë³´í˜¸)"""
    def keyify(s):
        return s.fillna("").astype(str)

    raw_keys = set(zip(keyify(raw.get("url", pd.Series(dtype=str))),
                       keyify(raw.get("title", pd.Series(dtype=str)))))

    mask = []
    for u, t in zip(keyify(new_df.get("url", pd.Series(dtype=str))),
                    keyify(new_df.get("title", pd.Series(dtype=str)))):
        mask.append((u, t) not in raw_keys)
    return new_df[pd.Series(mask, index=new_df.index)]


def append_only_raw(raw_path: str, new_batch_paths) -> pd.DataFrame:
    # ê¸°ì¡´ raw ë¡œë”©(í•œê¸€ ì»¬ëŸ¼ â†’ í‘œì¤€ ì»¬ëŸ¼ ì—­ë§¤í•‘)
    if os.path.exists(raw_path):
        raw = pd.read_csv(raw_path)
        raw_std = raw.rename(columns={
            "í‚¤ì›Œë“œ": "keyword",
            "ì œëª©": "title",
            "ì›ë¬¸ë§í¬": "url",
            "ë°œí–‰ì¼(KST)": "pub_date",
            "ìˆ˜ì§‘ì‹œê°(KST)": "collected_at",
            "ì¶œì²˜": "press",
        })
        raw = map_columns(raw_std)
    else:
        raw = pd.DataFrame(columns=list(COL_CANDIDATES.keys()))

    # ì‹ ê·œ ë°°ì¹˜ ë¡œë”©
    new_df = load_csvs(new_batch_paths)
    if new_df.empty:
        print("[INFO] ì‹ ê·œ ë°°ì¹˜ ì—†ìŒ.")
        return raw

    new_df = map_columns(new_df)

    # ìˆ˜ì§‘ì‹œê°: tz-aware(KST)ë¡œ ëª…ì‹œì  ê¸°ë¡
    if "collected_at" in new_df.columns:
        new_df["collected_at"] = new_df["collected_at"].fillna(pd.Timestamp.now(tz=KST_TZ))

    # 1ì°¨ ì¤‘ë³µ ì œê±°
    new_df = anti_join_by_url_title(raw, new_df)
    if new_df.empty:
        print("[INFO] ì¶”ê°€í•  ì‹ ê·œ ë ˆì½”ë“œê°€ ì—†ìŒ.")
        return raw

    return pd.concat([raw, new_df], ignore_index=True)


# ---------------------------------------------------------------------
# ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
# ---------------------------------------------------------------------
PREFERRED_DOMAINS = [
    "yna.co.kr", "news1.kr", "newsis.com", "sedaily.com", "hankyung.com",
    "mk.co.kr", "heraldcorp.com", "chosun.com", "joongang.co.kr", "hani.co.kr",
]


def domain_score(u: str) -> int:
    host = extract_domain(u)
    if not host:
        return 0
    for rank, dom in enumerate(reversed(PREFERRED_DOMAINS), start=1):
        if dom in host:
            return rank
    return 0


def build_title_clusters(df: pd.DataFrame, sim_threshold=0.85, n_neighbors=10):
    titles = df["title"].fillna("").astype(str)
    vec = TfidfVectorizer(analyzer="char_wb", ngram_range=(2, 4), min_df=1)
    X = vec.fit_transform(titles)
    nn = NearestNeighbors(metric="cosine", n_neighbors=min(n_neighbors, X.shape[0])).fit(X)
    distances, indices = nn.kneighbors(X)
    thr_dist = 1 - sim_threshold
    parent = list(range(len(df)))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for i in range(len(df)):
        for d, j in zip(distances[i], indices[i]):
            if i == j:
                continue
            if d <= thr_dist:
                union(i, j)

    groups = {}
    for i in range(len(df)):
        r = find(i)
        groups.setdefault(r, []).append(i)
    return groups


def choose_representative(df: pd.DataFrame, idxs: list[int]) -> int:
    subset = df.iloc[idxs].copy()
    subset["dom_score"] = subset["url"].apply(domain_score)
    subset["_date"] = pd.to_datetime(
        subset["pub_date"] if "pub_date" in subset.columns else subset["collected_at"],
        errors="coerce"
    )
    subset["title_len"] = subset["title"].astype(str).str.len()
    subset = subset.sort_values(by=["dom_score", "_date", "title_len"],
                                ascending=[False, True, False])
    return subset.index[0]


def dedupe_by_title_similarity(raw_df: pd.DataFrame, sim_threshold=0.85) -> pd.DataFrame:
    if raw_df.empty:
        return raw_df
    for c in ["title", "url"]:
        if c not in raw_df.columns:
            raw_df[c] = ""
    groups = build_title_clusters(raw_df, sim_threshold=sim_threshold)
    rep_indices = [choose_representative(raw_df, rows) for rows in groups.values()]
    deduped = raw_df.loc[sorted(set(rep_indices))].copy()
    deduped.reset_index(drop=True, inplace=True)
    return deduped


# ---------------------------------------------------------------------
# ì—”ë“œíˆ¬ì—”ë“œ
# ---------------------------------------------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--new-batch", nargs="+", default=["data/*.csv"], help="ì‹ ê·œ ë°°ì¹˜ CSV ê²½ë¡œ(ê¸€ë¡­ ê°€ëŠ¥)")
    ap.add_argument("--raw", default="data/news_raw.csv", help="append-only ì›ë³¸ CSV ê²½ë¡œ")
    ap.add_argument("--master", default="data/news_master.csv", help="ì¤‘ë³µ ì œê±°/ì •ì œ ê²°ê³¼ CSV ê²½ë¡œ")
    ap.add_argument("--sim-threshold", type=float, default=0.85, help="ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.85)")
    args = ap.parse_args()

    os.makedirs(os.path.dirname(args.raw), exist_ok=True)
    os.makedirs(os.path.dirname(args.master), exist_ok=True)

    raw_std = append_only_raw(args.raw, args.new_batch)
    master_std = dedupe_by_title_similarity(raw_std, sim_threshold=args.sim_threshold)

    # ì €ì¥ ì§ì „, 6ê°œ ì»¬ëŸ¼ë§Œ(ìˆœì„œ ê³ ì •)ìœ¼ë¡œ íˆ¬ì˜
    raw_out = format_output_columns(raw_std).loc[:, OUTPUT_ORDER]
    master_out = format_output_columns(master_std).loc[:, OUTPUT_ORDER]

    raw_out.to_csv(args.raw, index=False, encoding="utf-8-sig")
    print(f"[OK] raw ì €ì¥: {args.raw} (rows={len(raw_out)})")

    master_out.to_csv(args.master, index=False, encoding="utf-8-sig")
    print(f"[OK] master ì €ì¥: {args.master} (rows={len(master_out)})")


if __name__ == "__main__":
    main()
