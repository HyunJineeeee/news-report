# -*- coding: utf-8 -*-
"""
news_pipeline.py (ìµœì¢… ë²„ì „)
---------------------------------------
- append-only ì›ë³¸ ëˆ„ì (raw)
- ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° â†’ ëŒ€í‘œ ê¸°ì‚¬ 1ê±´ ì„ ì •(master)
- ì œëª© ê¸°ë°˜ Top í‚¤ì›Œë“œ Bar ì°¨íŠ¸ (ì£¼/ì›”/ë¶„ê¸°)
- âœ… ì €ìž¥ ì‹œ ì»¬ëŸ¼ ìˆœì„œ ê³ ì •:
    ["í‚¤ì›Œë“œ", "ì œëª©", "ì›ë¬¸ë§í¬", "ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)", "ì¶œì²˜"]
- âœ… ì°¨íŠ¸ ë””ìžì¸ ê°œì„  / í•œê¸€ í°íŠ¸(Nanum, Noto) ì ìš© / ê°’ ë¼ë²¨ í‘œì‹œ
"""

import argparse
import glob
import os
import re
from urllib.parse import urlparse

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors


# ---------------------------------------------------------------------
# 0. ê·¸ëž˜í”„ í…Œë§ˆ ë° í°íŠ¸ ì„¤ì •
# ---------------------------------------------------------------------
def setup_matplotlib_theme():
    """í°íŠ¸/ìŠ¤íƒ€ì¼ ê¸°ë³¸ ì„¸íŒ…"""
    preferred_fonts = ["NanumGothic", "Noto Sans CJK KR", "AppleGothic", "Malgun Gothic"]
    for fam in preferred_fonts:
        try:
            matplotlib.rcParams["font.family"] = fam
            break
        except Exception:
            continue
    matplotlib.rcParams["axes.unicode_minus"] = False
    plt.style.use("ggplot")
    matplotlib.rcParams.update({
        "figure.dpi": 150,
        "savefig.dpi": 300,
        "axes.titleweight": "bold",
        "axes.titlesize": 14,
        "axes.labelsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 11,
        "axes.edgecolor": "#DDDDDD",
        "grid.alpha": 0.4,
    })


# ---------------------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì • / ì»¬ëŸ¼ ì •ì˜
# ---------------------------------------------------------------------
KST_TZ = "Asia/Seoul"
OUTPUT_ORDER = ["í‚¤ì›Œë“œ", "ì œëª©", "ì›ë¬¸ë§í¬", "ë°œí–‰ì¼(KST)", "ìˆ˜ì§‘ì‹œê°(KST)", "ì¶œì²˜"]

COL_CANDIDATES = {
    "keyword": ["keyword", "í‚¤ì›Œë“œ"],
    "title": ["title", "subject", "headline", "ì œëª©"],
    "url": ["url", "link", "ê¸°ì‚¬ë§í¬", "news_url", "ì›ë¬¸ë§í¬"],
    "pub_date": ["pub_date", "date", "published_at", "publish_date", "ë°œí–‰ì¼", "ìž‘ì„±ì¼"],
    "press": ["press", "source", "publisher", "ì–¸ë¡ ì‚¬", "ë§¤ì²´", "ì¶œì²˜"],
    "collected_at": ["collected_at", "staged_at", "crawled_at", "ìˆ˜ì§‘ì¼", "ìˆ˜ì§‘ì¼ìž", "ìˆ˜ì§‘ì‹œê°"],
    "content": ["content", "summary", "ë³¸ë¬¸", "ìš”ì•½"],
}


# ---------------------------------------------------------------------
# 2. ìœ í‹¸ í•¨ìˆ˜
# ---------------------------------------------------------------------
def extract_domain(u: str) -> str:
    try:
        return urlparse(str(u)).netloc.lower()
    except Exception:
        return ""


def map_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ë ˆí¬ CSVì˜ ë‹¤ì–‘í•œ í—¤ë”ë¥¼ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ ì •ê·œí™”."""
    cols_lower = {c.lower(): c for c in df.columns}

    def choose(cands):
        for cand in cands:
            if cand in df.columns:
                return cand
            if cand.lower() in cols_lower:
                return cols_lower[cand.lower()]
        return None

    # 1) í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë¦¬ë„¤ìž„ / ì—†ìœ¼ë©´ ìƒì„±
    for std, cands in COL_CANDIDATES.items():
        chosen = choose(cands)
        if chosen is None:
            df[std] = pd.Series([np.nan] * len(df))
        else:
            if chosen != std:
                df.rename(columns={chosen: std}, inplace=True)

    # 2) ðŸ”§ ê°™ì€ ì´ë¦„ì˜ ì»¬ëŸ¼ì´ ë‘˜ ì´ìƒì´ë©´ í•˜ë‚˜ë¡œ ë³‘í•© (ë¨¼ì € ë‚˜ì˜¤ëŠ” ê°’ ìš°ì„ )
    #    ì˜ˆ: ['pub_date','pub_date'] ê°™ì€ ì¤‘ë³µ í—¤ë”
    if df.columns.duplicated().any():
        dup_names = set(df.columns[df.columns.duplicated(keep=False)])
        for name in dup_names:
            # í•´ë‹¹ ì´ë¦„ì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ëª¨ì•„ ì•žìª½ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            same_cols = [i for i, c in enumerate(df.columns) if c == name]
            merged = df.iloc[:, same_cols].bfill(axis=1).iloc[:, 0]
            # ê¸°ì¡´ ì¤‘ë³µ ì»¬ëŸ¼ë“¤ ì œê±° í›„ í•˜ë‚˜ë§Œ ë‚¨ê¹€
            df.drop(columns=[df.columns[i] for i in same_cols], inplace=True)
            df[name] = merged

    # 3) ë‚ ì§œ íŒŒì‹± (ì—¬ê¸°ì„œë¶€í„°ëŠ” ë‹¨ì¼ Series ë³´ìž¥)
    for dcol in ["pub_date", "collected_at"]:
        if dcol in df.columns:
            ser = df[dcol]
            # í˜¹ì‹œ ëª¨ë¥¼ DataFrame ë°©ì§€(ì´ì¤‘ ì•ˆì „ìž¥ì¹˜)
            if isinstance(ser, pd.DataFrame):
                ser = ser.bfill(axis=1).iloc[:, 0]
            df[dcol] = pd.to_datetime(ser, errors="coerce")

    # 4) press ì—†ìœ¼ë©´ URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
    if "press" in df.columns and "url" in df.columns:
        df["press"] = df["press"].fillna("")
        empty_press = df["press"].astype(str).str.strip().eq("")
        if empty_press.any():
            df.loc[empty_press, "press"] = df.loc[empty_press, "url"].apply(extract_domain)

    return df


def to_kst_ts(ts):
    if pd.isna(ts):
        return pd.NaT
    ts = pd.to_datetime(ts, errors="coerce")
    if ts is pd.NaT or ts is None:
        return pd.NaT
    try:
        if ts.tzinfo is None or ts.tzinfo.utcoffset(ts) is None:
            return ts.tz_localize(KST_TZ)
        else:
            return ts.tz_convert(KST_TZ)
    except Exception:
        return pd.NaT


def format_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ìµœì¢… ì €ìž¥ìš© ì»¬ëŸ¼ ìˆœì„œ/í˜•ì‹ ë³€í™˜"""
    out = pd.DataFrame(index=df.index.copy())
    out["í‚¤ì›Œë“œ"] = df.get("keyword", "").fillna("").astype(str)
    out["ì œëª©"] = df.get("title", "").astype(str)
    out["ì›ë¬¸ë§í¬"] = df.get("url", "").astype(str)
    out["ì¶œì²˜"] = df.get("press", "").astype(str)
    pub_kst = pd.to_datetime(df.get("pub_date", pd.NaT), errors="coerce").apply(to_kst_ts)
    col_kst = pd.to_datetime(df.get("collected_at", pd.NaT), errors="coerce").apply(to_kst_ts)
    out["ë°œí–‰ì¼(KST)"] = pub_kst.dt.strftime("%Y-%m-%d %H:%M").fillna("")
    out["ìˆ˜ì§‘ì‹œê°(KST)"] = col_kst.dt.strftime("%Y-%m-%d %H:%M").fillna("")
    return out.reindex(columns=OUTPUT_ORDER)


# ---------------------------------------------------------------------
# 3. ë°ì´í„° ì ìž¬ / append-only
# ---------------------------------------------------------------------
def load_csvs(patterns):
    paths = []
    for p in patterns:
        paths.extend(glob.glob(p))
    frames = []
    for path in sorted(set(paths)):
        try:
            df = pd.read_csv(path)
            frames.append(df)
        except Exception as e:
            print(f"[WARN] CSV ë¡œë“œ ì‹¤íŒ¨: {path} - {e}")
    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)


def anti_join_by_url_title(raw: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    def keyify(s):
        return s.fillna("").astype(str)

    raw_keys = set(zip(keyify(raw.get("url", pd.Series(dtype=str))),
                       keyify(raw.get("title", pd.Series(dtype=str)))))

    mask = []
    for u, t in zip(keyify(new_df.get("url", pd.Series(dtype=str))),
                    keyify(new_df.get("title", pd.Series(dtype=str)))):
        mask.append((u, t) not in raw_keys)
    return new_df[pd.Series(mask, index=new_df.index)]


def append_only_raw(raw_path: str, new_batch_paths) -> pd.DataFrame:
    if os.path.exists(raw_path):
        raw = pd.read_csv(raw_path)
        raw_std = raw.rename(columns={
            "í‚¤ì›Œë“œ": "keyword",
            "ì œëª©": "title",
            "ì›ë¬¸ë§í¬": "url",
            "ë°œí–‰ì¼(KST)": "pub_date",
            "ìˆ˜ì§‘ì‹œê°(KST)": "collected_at",
            "ì¶œì²˜": "press",
        })
        raw = map_columns(raw_std)
    else:
        raw = pd.DataFrame(columns=list(COL_CANDIDATES.keys()))

    new_df = load_csvs(new_batch_paths)
    if new_df.empty:
        print("[INFO] ì‹ ê·œ ë°°ì¹˜ ì—†ìŒ.")
        return raw

    new_df = map_columns(new_df)
    if "collected_at" in new_df.columns:
        new_df["collected_at"] = new_df["collected_at"].fillna(pd.Timestamp.now())

    new_df = anti_join_by_url_title(raw, new_df)
    if new_df.empty:
        print("[INFO] ì¶”ê°€í•  ì‹ ê·œ ë ˆì½”ë“œê°€ ì—†ìŒ.")
        return raw

    appended = pd.concat([raw, new_df], ignore_index=True)
    return appended


# ---------------------------------------------------------------------
# 4. ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
# ---------------------------------------------------------------------
PREFERRED_DOMAINS = [
    "yna.co.kr", "news1.kr", "newsis.com", "sedaily.com", "hankyung.com",
    "mk.co.kr", "heraldcorp.com", "chosun.com", "joongang.co.kr", "hani.co.kr",
]


def domain_score(u: str) -> int:
    host = extract_domain(u)
    if not host:
        return 0
    for rank, dom in enumerate(reversed(PREFERRED_DOMAINS), start=1):
        if dom in host:
            return rank
    return 0


def build_title_clusters(df: pd.DataFrame, sim_threshold=0.85, n_neighbors=10):
    titles = df["title"].fillna("").astype(str)
    vec = TfidfVectorizer(analyzer="char_wb", ngram_range=(2, 4), min_df=1)
    X = vec.fit_transform(titles)
    nn = NearestNeighbors(metric="cosine", n_neighbors=min(n_neighbors, X.shape[0])).fit(X)
    distances, indices = nn.kneighbors(X)
    thr_dist = 1 - sim_threshold
    parent = list(range(len(df)))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for i in range(len(df)):
        for d, j in zip(distances[i], indices[i]):
            if i == j:
                continue
            if d <= thr_dist:
                union(i, j)

    groups = {}
    for i in range(len(df)):
        r = find(i)
        groups.setdefault(r, []).append(i)
    return groups


def choose_representative(df: pd.DataFrame, idxs: list[int]) -> int:
    subset = df.iloc[idxs].copy()
    subset["dom_score"] = subset["url"].apply(domain_score)
    if "pub_date" in subset.columns:
        subset["_date"] = pd.to_datetime(subset["pub_date"], errors="coerce")
    else:
        subset["_date"] = pd.to_datetime(subset["collected_at"], errors="coerce")
    subset["title_len"] = subset["title"].astype(str).str.len()
    subset = subset.sort_values(by=["dom_score", "_date", "title_len"],
                                ascending=[False, True, False])
    return subset.index[0]


def dedupe_by_title_similarity(raw_df: pd.DataFrame,
                               sim_threshold=0.85) -> pd.DataFrame:
    if raw_df.empty:
        return raw_df
    for c in ["title", "url"]:
        if c not in raw_df.columns:
            raw_df[c] = ""
    groups = build_title_clusters(raw_df, sim_threshold=sim_threshold)
    rep_indices = [choose_representative(raw_df, rows) for rows in groups.values()]
    deduped = raw_df.loc[sorted(set(rep_indices))].copy()
    deduped.reset_index(drop=True, inplace=True)
    return deduped


# ---------------------------------------------------------------------
# 5. í‚¤ì›Œë“œ ë¶„ì„ / ì‹œê°í™”
# ---------------------------------------------------------------------
STOPWORDS = {
    "ì¼í•™ìŠµë³‘í–‰","ì§ì—…í›ˆë ¨","ê³ ìš©ë…¸ë™ë¶€","í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨",
    "ë„¤ì´ë²„","ë‹¤ìŒ","daum","naver","kr","com","net","co","news","ì—°í•©ë‰´ìŠ¤",
    "ê¸°ì‚¬","ì‚¬ì§„","ì˜ìƒ","ë³´ë„","ì†ë³´","ë‹¨ë…","ì¢…í•©","ì¸í„°ë·°",
    "ê¸°ìž","ê´€ë ¨","ì •ë¶€","ì‚¬ì—…","ì§€ì›","ì¶”ì§„"
}


def normalize_title_korean(text: str) -> list[str]:
    if not isinstance(text, str) or not text.strip():
        return []
    cleaned = re.sub(r"[^0-9A-Za-zê°€-íž£\s]", " ", text)
    toks = [t for t in re.split(r"\s+", cleaned.strip()) if t]
    out = []
    for t in toks:
        if len(t) < 2:
            continue
        if re.fullmatch(r"\d+", t):
            continue
        if re.fullmatch(r"20\d{2}", t):
            continue
        if re.fullmatch(r"[A-Za-z]{2,5}", t) and t.lower() in {"kr","com","net","co"}:
            continue
        if t in STOPWORDS:
            continue
        out.append(t)
    return out


def _prettify_axes(ax):
    for spine in ["top", "right"]:
        ax.spines[spine].set_visible(False)


def top_keywords_bar(df: pd.DataFrame, outdir: str, period: str = "W", top_k=25):
    if df.empty:
        return
    os.makedirs(outdir, exist_ok=True)

    if "pub_date" in df.columns and not df["pub_date"].isna().all():
        base_date = pd.to_datetime(df["pub_date"], errors="coerce")
    elif "collected_at" in df.columns and not df["collected_at"].isna().all():
        base_date = pd.to_datetime(df["collected_at"], errors="coerce")
    else:
        base_date = pd.Series([pd.Timestamp.now()] * len(df))

    tmp = df.copy()
    tmp["__date"] = base_date
    tmp = tmp.dropna(subset=["__date"])
    tmp["__period"] = tmp["__date"].dt.to_period(period).dt.to_timestamp()

    for pval, chunk in tmp.groupby("__period"):
        tokens = []
        for t in chunk["title"].fillna("").astype(str):
            tokens.extend(normalize_title_korean(t))
        if not tokens:
            continue
        vc = pd.Series(tokens).value_counts().head(top_k)

        h = max(5, 0.35 * len(vc) + 1.5)
        fig, ax = plt.subplots(figsize=(10, h))
        bars = ax.barh(vc.index[::-1], vc.values[::-1])

        for rect in bars:
            w = rect.get_width()
            ax.text(w + max(vc.values) * 0.01, rect.get_y() + rect.get_height()/2,
                    f"{int(w)}", va="center", ha="left", fontsize=10)

        ax.set_xlabel("ë¹ˆë„")
        ax.set_ylabel("")
        ax.set_title(f"Top Keywords ({period}) - {pval.date()}")
        ax.grid(axis="x")
        _prettify_axes(ax)
        ax.margins(x=0.05)

        fname = f"top_keywords_{period}_{pval.date()}.png"
        save_path = os.path.join(outdir, fname)
        plt.tight_layout()
        plt.savefig(save_path, bbox_inches="tight")
        plt.close(fig)
        print(f"[OK] ì €ìž¥: {save_path}")


# ---------------------------------------------------------------------
# 6. ì—”ë“œíˆ¬ì—”ë“œ ì‹¤í–‰
# ---------------------------------------------------------------------
def main():
    setup_matplotlib_theme()

    ap = argparse.ArgumentParser()
    ap.add_argument("--new-batch", nargs="+", default=["data/*.csv"],
                    help="ì‹ ê·œ ë°°ì¹˜ CSV ê²½ë¡œ(ê¸€ë¡­ ê°€ëŠ¥)")
    ap.add_argument("--raw", default="data/news_raw.csv",
                    help="append-only ì›ë³¸ CSV ê²½ë¡œ")
    ap.add_argument("--master", default="data/news_master.csv",
                    help="ì¤‘ë³µ ì œê±°/ì •ì œ ê²°ê³¼ CSV ê²½ë¡œ")
    ap.add_argument("--outdir", default="output",
                    help="ì°¨íŠ¸/ê²°ê³¼ë¬¼ ì¶œë ¥ í´ë”")
    ap.add_argument("--sim-threshold", type=float, default=0.85,
                    help="ì œëª© ìœ ì‚¬ë„ ìž„ê³„ê°’ (ê¸°ë³¸ 0.85)")
    args = ap.parse_args()

    os.makedirs(os.path.dirname(args.raw), exist_ok=True)
    os.makedirs(os.path.dirname(args.master), exist_ok=True)
    os.makedirs(args.outdir, exist_ok=True)

    raw_std = append_only_raw(args.raw, args.new_batch)
    master_std = dedupe_by_title_similarity(raw_std, sim_threshold=args.sim_threshold)

    raw_out = format_output_columns(raw_std)
    master_out = format_output_columns(master_std)

    raw_out.to_csv(args.raw, index=False, encoding="utf-8-sig")
    print(f"[OK] raw ì €ìž¥(ê³ ì • ì»¬ëŸ¼): {args.raw} (rows={len(raw_out)})")

    master_out.to_csv(args.master, index=False, encoding="utf-8-sig")
    print(f"[OK] master ì €ìž¥(ê³ ì • ì»¬ëŸ¼): {args.master} (rows={len(master_out)})")

    for period in ["W", "M", "Q"]:
        top_keywords_bar(master_std, args.outdir, period=period, top_k=25)


if __name__ == "__main__":
    main()
